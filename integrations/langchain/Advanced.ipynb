{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c05ad14",
   "metadata": {},
   "source": [
    "# Using LangChain middleware with ChatWriter\n",
    "[Middleware](https://docs.langchain.com/oss/python/langchain/middleware/overview) is a powerful feature in LangChain that lets you **control and customize the behavior of a ChatWriter agent at every step** of its execution.\n",
    "\n",
    "This guide demonstrates how to apply both **built-in and custom middleware** to an agent. You'll explore features such as PII detection, call limits, human-in-the-loop workflows, prompt transformation, and more. By following this guide, you'll learn how to **monitor agent behavior, modify tool usage, and integrate advanced logic** into your agent workflow.\n",
    "\n",
    "## Prerequisites\n",
    "Before getting started, you'll need:\n",
    "- A Python environment with **LangChain** installed\n",
    "- Access to a **ChatWriter-compatible model**\n",
    "- Any tools you plan to integrate with your agent\n",
    "\n",
    "## Setup\n",
    "Install LangChain if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-writer langchain langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83228aad-b12a-443b-aa39-186ffaef9bbb",
   "metadata": {},
   "source": [
    "Next, set the `WRITER_API_KEY` environment variable. We recommend setting it in a `.env` file in the root of your project, but this tutorial will set it in an environment variable if you don't have a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e069fa0-c83b-450d-a27a-59133939c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from writerai import Writer\n",
    "from langchain_writer import ChatWriter\n",
    "\n",
    "\n",
    "if not os.getenv(\"WRITER_API_KEY\"):\n",
    "    os.environ[\"WRITER_API_KEY\"] = getpass.getpass(\"Enter your Writer API key: \")\n",
    "\n",
    "chat = ChatWriter(model=\"palmyra-x5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c6fa5",
   "metadata": {},
   "source": [
    "## Example: Agent with built-in middleware\n",
    "\n",
    "List of all built-in [middlewares](https://docs.langchain.com/oss/python/langchain/middleware/built-in).\n",
    "\n",
    "We create an agent that uses ChatWriter and attach middleware that detects PII in inputs/outputs (via `PIIMiddleware`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_writer import ChatWriter\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "# Create agent with ChatWriter and PII detection middleware\n",
    "agent = create_agent(\n",
    "    model=chat,  # chat model instance\n",
    "    middleware=[\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True)  # redact emails in input\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea467b-cf01-4af4-af45-ceb016876a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"extract the email from the next message: 'Check if the email example@gmail.com exists'\"}]})\n",
    "for el in result[\"messages\"]:\n",
    "    print(el.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095bb4b",
   "metadata": {},
   "source": [
    "## Custom middleware example (logging / debugging hook)\n",
    "\n",
    "You can define your own middleware by subclassing or using hooks to run custom logic.  \n",
    "More info about decorators [here](https://docs.langchain.com/oss/python/langchain/middleware/custom#decorator-based-middleware).  \n",
    "\n",
    "This is useful for logging, analytics, custom validations, formatting, and other custom behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelRequest, dynamic_prompt\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def context_aware_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Middleware that handles conversation context.\n",
    "    \"\"\"\n",
    "    message_count = len(request.messages)\n",
    "    print(f\"Context Middleware: messages={message_count}\")\n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "    if message_count >= 2:\n",
    "        prompt_parts.append(\"This is a long conversation - be extra concise.\")\n",
    "        print(\" State Context: Long conversation detected\")\n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    middleware=[context_aware_prompt]\n",
    ")\n",
    "\n",
    "\n",
    "res2 = agent.invoke({\"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Give me exampples for the most powerfull quantcopmucters\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me exampples for the most powerfull quantcopmucters\"}\n",
    "]})\n",
    "for el in res2[\"messages\"]:\n",
    "    print(el.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e736d34-abe9-4229-93b0-353d8e44617b",
   "metadata": {},
   "source": [
    "## Context engineering in agents\n",
    "\n",
    "### Overview\n",
    "\n",
    "The hardest part of building reliable agents (or any LLM-powered application) is not just picking a strong model â€” itâ€™s ensuring the right context is passed to that model for each call. When agents fail in real-world use, it is often because the context was wrong, incomplete, or poorly formatted.\n",
    "\n",
    "**Context engineering** means deliberately constructing and managing what context (information, state, memory, configuration) is made available to the LLM â€” and how â€” so that tasks succeed reliably.  \n",
    "\n",
    "With LangChain + LangGraph, context is first-class: you get **runtime context**, **state**, and **persistent store**, with full control over how these feed into model calls, tools, prompts, and middleware.  \n",
    "More info [here](https://docs.langchain.com/oss/python/concepts/context).\n",
    "\n",
    "Now, let's see how static runtime context works â€” passing fixed configuration (like role, style, or preferences) to the agent so it influences every model call without changing during execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0b52a-f157-43d9-a71e-44963486361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "# Define context schema for agent\n",
    "@dataclass\n",
    "class ContextSchema:\n",
    "    communication_style: str  # e.g., \"Junior Analyst\", \"Portfolio Manager\"\n",
    "\n",
    "# Dynamic prompt middleware that adapts to user communication style\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:  \n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "    # Get communication style from runtime context\n",
    "    communication_style = request.runtime.context.communication_style\n",
    "\n",
    "    if communication_style == \"Junior Analyst\":\n",
    "        prompt_parts.append(\n",
    "            \"Provide detailed, step-by-step breakdowns and rationales.\"\n",
    "        )\n",
    "    elif communication_style == \"Portfolio Manager\":\n",
    "        prompt_parts.append(\n",
    "            \"Provide concise summaries emphasizing decision-critical metrics and outcomes.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_parts.append(\"Provide balanced responses.\")\n",
    "    \n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    print(f\"***Final prompt: {final_prompt}***\")\n",
    "    return final_prompt\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"web_search\",\n",
    "    \"function\": {}\n",
    "}]\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=tools,\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=ContextSchema,\n",
    ")\n",
    "\n",
    "res3 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze Tesla stock performance in 2024 and explain the key drivers step by step.\"}]},\n",
    "    context=ContextSchema(communication_style=\"Junior Analyst\")  \n",
    ")\n",
    "for el in res3[\"messages\"]:\n",
    "    print(f\"{el.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a7949-953c-4484-9297-d374a4cce8b1",
   "metadata": {},
   "source": [
    "### Example: Context-aware prompt using persistent store and runtime context\n",
    "\n",
    "This example shows how to personalize agent behavior using **user-specific preferences** stored in a **persistent store**, based on a unique `user_id`.\n",
    "\n",
    "The `personalized_prompt` middleware:\n",
    "- Accesses `user_id` from the runtime context  \n",
    "- Retrieves user preferences (e.g., communication style) from the store  \n",
    "- Dynamically adjusts the system prompt based on stored preferences  \n",
    "- Falls back to a balanced style if no valid preference is found  \n",
    "\n",
    "This enables:\n",
    " - Personalized responses per user  \n",
    " - Persistent preference storage across sessions  \n",
    " - Automatic adaptation of prompt behavior using runtime context  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e24cd-17dc-4fbd-b760-bae641647b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserState:\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Middleware to generate prompts based on stored user preferences.\n",
    "    \"\"\"\n",
    "    store = request.runtime.store  # Access persistent storage\n",
    "    user_id = str(request.runtime.context.user_id)  # Get current user ID\n",
    "    user_prefs = store.get((\"preferences\",), user_id)  # Fetch preferences for user\n",
    "\n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "\n",
    "    if not user_prefs or not user_prefs.value.get(\"communication_style\"):\n",
    "        prompt_parts.append(\n",
    "            \"Provide balanced responses\"\n",
    "        )\n",
    "    elif user_prefs.value.get(\"communication_style\") == \"Junior Analyst\":\n",
    "         prompt_parts.append(\n",
    "                \"Provide detailed, step-by-step breakdowns and rationales for all responses.\"\n",
    "            )\n",
    "    elif user_prefs.value.get(\"communication_style\") == \"Portfolio Manager\":\n",
    "        prompt_parts.append(\n",
    "                \"Provide concise summaries emphasizing decision-critical metrics and outcomes.\"\n",
    "            )\n",
    "    else:\n",
    "        prefs = user_prefs.value.get(\"communication_style\")\n",
    "        print(f\"Found unexpected preference - {prefs}\")\n",
    "        prompt_parts.append(\n",
    "            \"Provide balanced responses\"\n",
    "        )\n",
    "       \n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    print(f\"***Final prompt: {final_prompt}***\")\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e8e84-b9d4-46b3-b431-71026428623e",
   "metadata": {},
   "source": [
    "### Description â€” agent with persistent per-user preferences\n",
    "\n",
    "This code creates a LangChain agent that uses a persistent in-memory store to apply **per-user prompt personalization** at runtime.\n",
    "\n",
    "**Key behaviors**\n",
    "- The `personalized_prompt` middleware reads `user_id` from `context` and looks up that userâ€™s preferences in the store to adapt the system prompt (e.g., detailed step-by-step vs. concise summary).\n",
    "- Using `InMemoryStore` keeps preferences in-process for this example; swap for a persistent store (Redis/DB) in production.\n",
    "\n",
    "**Notes**\n",
    "- `user_id` used when calling `agent.invoke(..., context=UserState(user_id=user_id))` controls which stored preferences apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ee71f-f19c-4b78-96d0-3ca74f4cb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=tools,\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=UserState,\n",
    "    store=InMemoryStore()\n",
    ")\n",
    "\n",
    "# Reference to the agent's store\n",
    "store = agent.store\n",
    "\n",
    "# Example user IDs\n",
    "user_id = 1\n",
    "\n",
    "# Store user preferences in the persistent store\n",
    "store.put((\"preferences\",), user_id, {\"communication_style\": \"Junior Analyst\"})\n",
    "store.put((\"preferences\",), 2, {\"communication_style\": \"Portfolio Manager\"})\n",
    "\n",
    "\n",
    "# Invoke the agent for a specific user with runtime context\n",
    "res3 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze Tesla stock performance in 2024 and explain the key drivers step by step.\"}]},\n",
    "    context=UserState(user_id=user_id)  # runtime context determines which preferences apply \n",
    ")\n",
    "\n",
    "\n",
    "for el in res3[\"messages\"]:\n",
    "    print(f\"{el.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598719b-d62b-459c-8004-dc6fcaae9287",
   "metadata": {},
   "source": [
    "## LangChain + Writer MCP cookbook: Building research agents with streamable HTTP \n",
    "\n",
    "This section demonstrates:\n",
    "\n",
    "- How the Writer MCP client works.\n",
    "- How to register and use tools.\n",
    "- Running test queries.\n",
    "- Extending the research tool.\n",
    "\n",
    "First, install the required library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584ab53-a060-4d66-b9d8-32c549264829",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bda2e-f8dc-4174-a6b0-ea7e3b626a46",
   "metadata": {},
   "source": [
    "## Building a custom MCP client with LangChain\n",
    "\n",
    "Here, we import the necessary libraries for creating a research agent with LangChain and Writer MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948248cb-0eec-4180-9387-4ce8f9050b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain_writer import ChatWriter\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0804cb0-b7ba-479c-aced-a95527851b85",
   "metadata": {},
   "source": [
    "## Creating an asynchronous research task\n",
    "\n",
    "This code defines an asynchronous task for querying the Writer MCP client. \n",
    "- It sets up a system prompt to guide the research assistant.\n",
    "- Initializes the MCP client with a streamable HTTP transport.\n",
    "- Loads the ChatWriter model.\n",
    "- Fetches available tools and creates an agent.\n",
    "- Streams and prints responses in real time, accumulating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3565a1-566a-48c0-8d57-bc9a28a9d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _async_task(query: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You are a research assistant with access to Writer documentation.\n",
    "    When answering questions:\n",
    "    1. Focus on relevant documentation only.\n",
    "    2. Provide clear, concise bullet points.\n",
    "    3. Avoid opinions or unnecessary elaboration.\n",
    "    \"\"\"\n",
    "\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"research\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"https://dev.writer.com/mcp\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = ChatWriter(model=\"palmyra-x5\")\n",
    "    tools = await client.get_tools()\n",
    "    agent = create_agent(model, tools, system_prompt=system_prompt)\n",
    "\n",
    "    final_response = \"\"\n",
    "\n",
    "    # Stream\n",
    "    async for chunk in agent.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        for step, data in chunk.items():\n",
    "            text = data['messages'][-1].content_blocks[0].get('text', \"\")\n",
    "            print(f\"{text[:100]}...\")\n",
    "            final_response += text\n",
    "\n",
    "    return f\"[Research] Information from internal documentation: {final_response[:800]}...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3dc87d-742f-497e-ba88-8d44841a5db8",
   "metadata": {},
   "source": [
    "## Testing the research\n",
    "\n",
    "This code tests the `_async_task` function with a sample query:\n",
    "\n",
    "- Demonstrates how to format the query with tone and style instructions.\n",
    "- Runs the async task directly in a Jupyter environment.\n",
    "- Prints the streamed response and the final accumulated result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808284c-5f8b-4050-b9f4-b3d9e1252c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_query = (\n",
    "    \"I'm interested in no-code apps. \"\n",
    "    \"Use a friendly tone and concise bullet points with keyâ€“value pairs only.\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Running _async_task directly for testing...\\n\")\n",
    "result = await _async_task(test_query)\n",
    "print(\"\\n\\nâœ… Response:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec1620-27dc-477f-aee0-e08cf6dc63a6",
   "metadata": {},
   "source": [
    "## Adding the MCP client as a research tool\n",
    "\n",
    "In this step, we define a custom tool that integrates the Writer MCP client into our LangChain agent.  \n",
    "\n",
    "This tool allows the agent to:\n",
    "\n",
    "- Query Writer documentation and internal resources.\n",
    "- Provide concise, actionable insights in response to user queries.\n",
    "- Support research tasks such as product comparisons, feature analysis, and platform evaluation.\n",
    "\n",
    "The tool is annotated with the `@tool` decorator from LangChain, giving it a name and a description that will be visible to the agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f9079-0b9b-4779-91e1-0f41000d93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(\n",
    "    \"Research\",\n",
    "    description=(\n",
    "        \"Research documentation, products, or platform comparisons using Writer's MCP client and Palmyra X5 model. \"\n",
    "        \"Use this tool when the user asks to compare, analyze, or get detailed insights on products, features, or platforms. \"\n",
    "        \"Required: query.\"\n",
    "    ),\n",
    ")\n",
    "async def writer_research_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query Writer documentation and return concise, actionable insights.\n",
    "\n",
    "    Parameters:\n",
    "        - query (str): The question or topic to research.\n",
    "\n",
    "    \"\"\"\n",
    "    return await _async_task(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34721b-11f7-4ce9-9537-0f555fdd085a",
   "metadata": {},
   "source": [
    "In th next section, we build a custom customer support agent using LangChain and the Writer MCP client. The `writer_research_tool` will be included as a tool into a new agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f35b9-c345-478c-939d-7ae7240729b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def create_customer_support_agent():\n",
    "    chat = ChatWriter(model=\"palmyra-x5\")\n",
    "\n",
    "    agent = create_agent(\n",
    "        model=chat,\n",
    "        tools=[\n",
    "            writer_research_tool\n",
    "        ],\n",
    "        system_prompt=(\n",
    "            \"You are a customer support assistant for an e-commerce platform. Use tools to handle user requests.\\n\\n\"\n",
    "            \"Tool Usage:\\n\"\n",
    "            \"[Research] - Research documentation or products using Writer's MCP client and Palmyra X5 model\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Ask for missing required parameters before calling tools.\\n\"\n",
    "        ),\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "# Example query\n",
    "test_query = \"tell me about development platform and give the most popular no-code apps\"\n",
    "\n",
    "agent = create_customer_support_agent()\n",
    "\n",
    "result = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": test_query}]}\n",
    ")\n",
    "\n",
    "for el in result[\"messages\"]:\n",
    "    print(f\"{el.type}: {el.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162d2c6-2d62-4dc5-b384-652d858dd3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
