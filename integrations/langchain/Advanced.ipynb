{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c05ad14",
   "metadata": {},
   "source": [
    "# Using LangChain Middleware with ChatWriter\n",
    "[Middleware](https://docs.langchain.com/oss/python/langchain/middleware/overview) is a powerful feature in LangChain that lets you **control and customize the behavior of a ChatWriter agent at every step** of its execution.\n",
    "\n",
    "This guide demonstrates how to apply both **built-in and custom middleware** to an agent. You'll explore features such as PII detection, call limits, human-in-the-loop workflows, prompt transformation, and more. By following this guide, you'll learn how to **monitor agent behavior, modify tool usage, and integrate advanced logic** into your agent workflow.\n",
    "\n",
    "## Prerequisites\n",
    "Before getting started, you'll need:\n",
    "- A Python environment with **LangChain** installed\n",
    "- Access to a **ChatWriter-compatible model**\n",
    "- Any tools you plan to integrate with your agent\n",
    "\n",
    "## Setup\n",
    "Install LangChain if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-writer langchain langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83228aad-b12a-443b-aa39-186ffaef9bbb",
   "metadata": {},
   "source": [
    "Next, set the `WRITER_API_KEY` environment variable. We recommend setting it in a `.env` file in the root of your project, but this tutorial will set it in an environment variable if you don't have a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e069fa0-c83b-450d-a27a-59133939c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from writerai import Writer\n",
    "from langchain_writer import ChatWriter\n",
    "\n",
    "\n",
    "if not os.getenv(\"WRITER_API_KEY\"):\n",
    "    os.environ[\"WRITER_API_KEY\"] = getpass.getpass(\"Enter your Writer API key: \")\n",
    "\n",
    "chat = ChatWriter(model=\"palmyra-x5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c6fa5",
   "metadata": {},
   "source": [
    "## âœ… Example: Agent with Built-in Middleware\n",
    "\n",
    "List of all built-in [middlewares](https://docs.langchain.com/oss/python/langchain/middleware/built-in).\n",
    "\n",
    "We create an agent that uses ChatWriter and attach middleware that detects PII in inputs/outputs (via `PIIMiddleware`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_writer import ChatWriter\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "# Create agent with ChatWriter and PII detection middleware\n",
    "agent = create_agent(\n",
    "    model=chat,  # chat model instance\n",
    "    middleware=[\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True)  # redact emails in input\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea467b-cf01-4af4-af45-ceb016876a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"extract the email from the next message: 'Check if the email example@gmail.com exists'\"}]})\n",
    "for el in result[\"messages\"]:\n",
    "    print(el.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095bb4b",
   "metadata": {},
   "source": [
    "## ðŸ›  Custom Middleware Example (Logging / Debugging Hook)\n",
    "\n",
    "You can define your own middleware by subclassing or using hooks to run custom logic.  \n",
    "More info about decorators [here](https://docs.langchain.com/oss/python/langchain/middleware/custom#decorator-based-middleware).  \n",
    "\n",
    "This is useful for logging, analytics, custom validations, formatting, and other custom behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelRequest, dynamic_prompt\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def context_aware_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Middleware that handles conversation context.\n",
    "    \"\"\"\n",
    "    message_count = len(request.messages)\n",
    "    print(f\"Context Middleware: messages={message_count}\")\n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "    if message_count >= 2:\n",
    "        prompt_parts.append(\"This is a long conversation - be extra concise.\")\n",
    "        print(\" State Context: Long conversation detected\")\n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    middleware=[context_aware_prompt]\n",
    ")\n",
    "\n",
    "\n",
    "res2 = agent.invoke({\"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Give me exampples for the most powerfull quantcopmucters\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me exampples for the most powerfull quantcopmucters\"}\n",
    "]})\n",
    "for el in res2[\"messages\"]:\n",
    "    print(el.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e736d34-abe9-4229-93b0-353d8e44617b",
   "metadata": {},
   "source": [
    "# Context Engineering in Agents\n",
    "\n",
    "## Overview\n",
    "\n",
    "The hardest part of building reliable agents (or any LLM-powered application) is not just picking a strong model â€” itâ€™s ensuring the right context is passed to that model for each call. When agents fail in real-world use, it is often because the context was wrong, incomplete, or poorly formatted.\n",
    "\n",
    "**Context engineering** means deliberately constructing and managing what context (information, state, memory, configuration) is made available to the LLM â€” and how â€” so that tasks succeed reliably.  \n",
    "\n",
    "With LangChain + LangGraph, context is first-class: you get **runtime context**, **state**, and **persistent store**, with full control over how these feed into model calls, tools, prompts, and middleware.  \n",
    "More info [here](https://docs.langchain.com/oss/python/concepts/context).\n",
    "\n",
    "Now, let's see how static runtime context works â€” passing fixed configuration (like role, style, or preferences) to the agent so it influences every model call without changing during execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0b52a-f157-43d9-a71e-44963486361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "# Define context schema for agent\n",
    "@dataclass\n",
    "class ContextSchema:\n",
    "    communication_style: str  # e.g., \"Junior Analyst\", \"Portfolio Manager\"\n",
    "\n",
    "# Dynamic prompt middleware that adapts to user communication style\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:  \n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "    # Get communication style from runtime context\n",
    "    communication_style = request.runtime.context.communication_style\n",
    "\n",
    "    if communication_style == \"Junior Analyst\":\n",
    "        prompt_parts.append(\n",
    "            \"Provide detailed, step-by-step breakdowns and rationales.\"\n",
    "        )\n",
    "    elif communication_style == \"Portfolio Manager\":\n",
    "        prompt_parts.append(\n",
    "            \"Provide concise summaries emphasizing decision-critical metrics and outcomes.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_parts.append(\"Provide balanced responses.\")\n",
    "    \n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    print(f\"***Final prompt: {final_prompt}***\")\n",
    "    return final_prompt\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"web_search\",\n",
    "    \"function\": {}\n",
    "}]\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=tools,\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=ContextSchema,\n",
    ")\n",
    "\n",
    "res3 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze Tesla stock performance in 2024 and explain the key drivers step by step.\"}]},\n",
    "    context=ContextSchema(communication_style=\"Junior Analyst\")  \n",
    ")\n",
    "for el in res3[\"messages\"]:\n",
    "    print(f\"{el.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a7949-953c-4484-9297-d374a4cce8b1",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Example: Context-Aware Prompt Using Persistent Store and Runtime Context\n",
    "\n",
    "This example shows how to personalize agent behavior using **user-specific preferences** stored in a **persistent store**, based on a unique `user_id`.\n",
    "\n",
    "The `personalized_prompt` middleware:\n",
    "- Accesses `user_id` from the runtime context  \n",
    "- Retrieves user preferences (e.g., communication style) from the store  \n",
    "- Dynamically adjusts the system prompt based on stored preferences  \n",
    "- Falls back to a balanced style if no valid preference is found  \n",
    "\n",
    "This enables:\n",
    " - Personalized responses per user  \n",
    " - Persistent preference storage across sessions  \n",
    " - Automatic adaptation of prompt behavior using runtime context  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e24cd-17dc-4fbd-b760-bae641647b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserState:\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Middleware to generate prompts based on stored user preferences.\n",
    "    \"\"\"\n",
    "    store = request.runtime.store  # Access persistent storage\n",
    "    user_id = str(request.runtime.context.user_id)  # Get current user ID\n",
    "    user_prefs = store.get((\"preferences\",), user_id)  # Fetch preferences for user\n",
    "\n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "\n",
    "    if not user_prefs or not user_prefs.value.get(\"communication_style\"):\n",
    "        prompt_parts.append(\n",
    "            \"Provide balanced responses\"\n",
    "        )\n",
    "    elif user_prefs.value.get(\"communication_style\") == \"Junior Analyst\":\n",
    "         prompt_parts.append(\n",
    "                \"Provide detailed, step-by-step breakdowns and rationales for all responses.\"\n",
    "            )\n",
    "    elif user_prefs.value.get(\"communication_style\") == \"Portfolio Manager\":\n",
    "        prompt_parts.append(\n",
    "                \"Provide concise summaries emphasizing decision-critical metrics and outcomes.\"\n",
    "            )\n",
    "    else:\n",
    "        prefs = user_prefs.value.get(\"communication_style\")\n",
    "        print(f\"Found unexpected preference - {prefs}\")\n",
    "        prompt_parts.append(\n",
    "            \"Provide balanced responses\"\n",
    "        )\n",
    "       \n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    print(f\"***Final prompt: {final_prompt}***\")\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e8e84-b9d4-46b3-b431-71026428623e",
   "metadata": {},
   "source": [
    "### Description â€” Agent with persistent per-user preferences\n",
    "\n",
    "This code creates a LangChain agent that uses a persistent in-memory store to apply **per-user prompt personalization** at runtime.\n",
    "\n",
    "**Key behaviors**\n",
    "- The `personalized_prompt` middleware reads `user_id` from `context` and looks up that userâ€™s preferences in the store to adapt the system prompt (e.g., detailed step-by-step vs. concise summary).\n",
    "- Using `InMemoryStore` keeps preferences in-process for this example; swap for a persistent store (Redis/DB) in production.\n",
    "\n",
    "**Notes**\n",
    "- `user_id` used when calling `agent.invoke(..., context=UserState(user_id=user_id))` controls which stored preferences apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ee71f-f19c-4b78-96d0-3ca74f4cb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=tools,\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=UserState,\n",
    "    store=InMemoryStore()\n",
    ")\n",
    "\n",
    "# Reference to the agent's store\n",
    "store = agent.store\n",
    "\n",
    "# Example user IDs\n",
    "user_id = 1\n",
    "\n",
    "# Store user preferences in the persistent store\n",
    "store.put((\"preferences\",), user_id, {\"communication_style\": \"Junior Analyst\"})\n",
    "store.put((\"preferences\",), 2, {\"communication_style\": \"Portfolio Manager\"})\n",
    "\n",
    "\n",
    "# Invoke the agent for a specific user with runtime context\n",
    "res3 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze Tesla stock performance in 2024 and explain the key drivers step by step.\"}]},\n",
    "    context=UserState(user_id=user_id)  # runtime context determines which preferences apply \n",
    ")\n",
    "\n",
    "\n",
    "for el in res3[\"messages\"]:\n",
    "    print(f\"{el.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598719b-d62b-459c-8004-dc6fcaae9287",
   "metadata": {},
   "source": [
    "#  LangChain + Writer MCP Cookbook: Building Research Agents with Streamable HTTP  \n",
    "This section demonstrates:\n",
    "\n",
    "- How the Writer MCP client works.\n",
    "- How to register and use tools.\n",
    "- Running test queries.\n",
    "- Extending the research tool.\n",
    "\n",
    "First, let's install the required library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584ab53-a060-4d66-b9d8-32c549264829",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bda2e-f8dc-4174-a6b0-ea7e3b626a46",
   "metadata": {},
   "source": [
    "## Building a Custom MCP Client With LangChain\n",
    "\n",
    "Here, we import the necessary libraries for creating a research agent with LangChain and Writer MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948248cb-0eec-4180-9387-4ce8f9050b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain_writer import ChatWriter\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0804cb0-b7ba-479c-aced-a95527851b85",
   "metadata": {},
   "source": [
    "## Creating an Asynchronous Research Task\n",
    "\n",
    "This code defines an asynchronous task for querying the Writer MCP client. \n",
    "- It sets up a system prompt to guide the research assistant.\n",
    "- Initializes the MCP client with a streamable HTTP transport.\n",
    "- Loads the ChatWriter model.\n",
    "- Fetches available tools and creates an agent.\n",
    "- Streams and prints responses in real time, accumulating the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3565a1-566a-48c0-8d57-bc9a28a9d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _async_task(query: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "    You are a research assistant with access to Writer documentation.\n",
    "    When answering questions:\n",
    "    1. Focus on relevant documentation only.\n",
    "    2. Provide clear, concise bullet points.\n",
    "    3. Avoid opinions or unnecessary elaboration.\n",
    "    \"\"\"\n",
    "\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"research\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"https://dev.writer.com/mcp\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = ChatWriter(model=\"palmyra-x5\")\n",
    "    tools = await client.get_tools()\n",
    "    agent = create_agent(model, tools, system_prompt=system_prompt)\n",
    "\n",
    "    final_response = \"\"\n",
    "\n",
    "    # Stream\n",
    "    async for chunk in agent.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        for step, data in chunk.items():\n",
    "            text = data['messages'][-1].content_blocks[0].get('text', \"\")\n",
    "            print(f\"{text[:100]}...\")\n",
    "            final_response += text\n",
    "\n",
    "    return f\"[Research] Information from internal documentation: {final_response[:800]}...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3dc87d-742f-497e-ba88-8d44841a5db8",
   "metadata": {},
   "source": [
    "## Testing the Research\n",
    "\n",
    "This code tests the `_async_task` function with a sample query:\n",
    "\n",
    "- Demonstrates how to format the query with tone and style instructions.\n",
    "- Runs the async task directly in a Jupyter environment.\n",
    "- Prints the streamed response and the final accumulated result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808284c-5f8b-4050-b9f4-b3d9e1252c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_query = (\n",
    "    \"I'm interested in no-code apps. \"\n",
    "    \"Use a friendly tone and concise bullet points with keyâ€“value pairs only.\"\n",
    ")\n",
    "\n",
    "print(\"ðŸ” Running _async_task directly for testing...\\n\")\n",
    "result = await _async_task(test_query)\n",
    "print(\"\\n\\nâœ… Response:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec1620-27dc-477f-aee0-e08cf6dc63a6",
   "metadata": {},
   "source": [
    "## Adding the MCP Client as a Research Tool\n",
    "\n",
    "In this step, we define a custom tool that integrates the Writer MCP client into our LangChain agent.  \n",
    "\n",
    "This tool allows the agent to:\n",
    "\n",
    "- Query Writer documentation and internal resources.\n",
    "- Provide concise, actionable insights in response to user queries.\n",
    "- Support research tasks such as product comparisons, feature analysis, and platform evaluation.\n",
    "\n",
    "The tool is annotated with the `@tool` decorator from LangChain, giving it a name and a description that will be visible to the agent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024f9079-0b9b-4779-91e1-0f41000d93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(\n",
    "    \"Research\",\n",
    "    description=(\n",
    "        \"Research documentation, products, or platform comparisons using Writer's MCP client and Palmyra X5 model. \"\n",
    "        \"Use this tool when the user asks to compare, analyze, or get detailed insights on products, features, or platforms. \"\n",
    "        \"Required: query.\"\n",
    "    ),\n",
    ")\n",
    "async def writer_research_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query Writer documentation and return concise, actionable insights.\n",
    "\n",
    "    Parameters:\n",
    "        - query (str): The question or topic to research.\n",
    "\n",
    "    \"\"\"\n",
    "    return await _async_task(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34721b-11f7-4ce9-9537-0f555fdd085a",
   "metadata": {},
   "source": [
    "In th next section, we build a custom customer support agent using LangChain and the Writer MCP client. The `writer_research_tool` will be included as a tool into a new agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55f35b9-c345-478c-939d-7ae7240729b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "[\"Title: Invoke no-code agents via the API\\nLink: https://dev.writer.com/home/applications\\nContent:...\n",
      "Here are some key points about Writer's development platform and popular no-code applications:\n",
      "\n",
      "**De...\n",
      "Here are some popular no-code development platforms and apps:\n",
      "\n",
      "* **Web Development:**\n",
      "\t+ Bubble\n",
      "\t+ W...\n",
      "[\"Title: Invoke no-code agents via the API\\nLink: https://dev.writer.com/home/applications\\nContent:...\n",
      "Here are some key points about no-code development platforms and apps based on the Writer AI Studio ...\n",
      "human: tell me about development platform and give the most popular no-code apps\n",
      "ai: \n",
      "tool: [Research] Information from internal documentation: [\"Title: Invoke no-code agents via the API\\nLink: https://dev.writer.com/home/applications\\nContent: No-code applications are now called no-code agents . The Applications API , which you can use to programmatically interact with no-code agents, still uses the term application to minimize breaking changes. The Applications API allows you to turn deployed no-code agents into microservices, which can also be used as tools in tool calling . Business users can define inputs, prompts, and outputs, and developers can then add them to other applications, UIs, or services. This guide shows how to use the /applications endpoint to generate content from no-code agents. If you do not have a deployed no-code agent, follow the guides to build an agent with text generation or research capabilities in AI ...\n",
      "ai: To provide information on the development platform and the most popular no-code apps, I'll need to refine my research query.\n",
      "\n",
      "\n",
      "tool: [Research] Information from internal documentation: Here are some popular no-code development platforms and apps:\n",
      "\n",
      "* **Web Development:**\n",
      "\t+ Bubble\n",
      "\t+ Webflow\n",
      "\t+ Wix\n",
      "\t+ Squarespace\n",
      "* **Mobile App Development:**\n",
      "\t+ Adalo\n",
      "\t+ Appy Pie\n",
      "\t+ GoodBarber\n",
      "\t+ Glide\n",
      "* **Automation and Integration:**\n",
      "\t+ Zapier\n",
      "\t+ Automator (formerly known as Microsoft Power Automate)\n",
      "\t+ n8n.io\n",
      "\t+ Integromat\n",
      "* **Database and Productivity:**\n",
      "\t+ Airtable\n",
      "\t+ Notion\n",
      "\t+ Coda\n",
      "\t+ Google App Script (for Google Workspace)\n",
      "* **Other Notable Platforms:**\n",
      "\t+ OutSystems\n",
      "\t+ Mendix\n",
      "\t+ Microsoft Power Apps\n",
      "\n",
      "These platforms allow users to create applications, automate workflows, and build integrations without requiring extensive coding knowledge.\n",
      "\n",
      "To learn more about these platforms and their capabilities, I can search the Writer AI Studio knowledge base. For example, you can ask me to \"...\n",
      "ai: Some popular no-code development platforms include:\n",
      "\n",
      "### Web Development:\n",
      "* Bubble\n",
      "* Webflow\n",
      "* Wix\n",
      "* Squarespace\n",
      "\n",
      "### Mobile App Development:\n",
      "* Adalo\n",
      "* Appy Pie\n",
      "* GoodBarber\n",
      "* Glide\n",
      "\n",
      "### Automation and Integration:\n",
      "* Zapier\n",
      "* Automator (formerly known as Microsoft Power Automate)\n",
      "* n8n.io\n",
      "* Integromat\n",
      "\n",
      "### Database and Productivity:\n",
      "* Airtable\n",
      "* Notion\n",
      "* Coda\n",
      "* Google App Script (for Google Workspace)\n",
      "\n",
      "### Other Notable Platforms:\n",
      "* OutSystems\n",
      "* Mendix\n",
      "* Microsoft Power Apps\n",
      "\n",
      "These platforms enable users to create applications, automate workflows, and build integrations without requiring extensive coding knowledge.\n",
      "\n",
      "Would you like more information on any of these platforms?\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "def create_customer_support_agent():\n",
    "    chat = ChatWriter(model=\"palmyra-x5\")\n",
    "\n",
    "    agent = create_agent(\n",
    "        model=chat,\n",
    "        tools=[\n",
    "            writer_research_tool\n",
    "        ],\n",
    "        system_prompt=(\n",
    "            \"You are a customer support assistant for an e-commerce platform. Use tools to handle user requests.\\n\\n\"\n",
    "            \"Tool Usage:\\n\"\n",
    "            \"[Research] - Research documentation or products using Writer's MCP client and Palmyra X5 model\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Ask for missing required parameters before calling tools.\\n\"\n",
    "        ),\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "# Example query\n",
    "test_query = \"tell me about development platform and give the most popular no-code apps\"\n",
    "\n",
    "agent = create_customer_support_agent()\n",
    "\n",
    "result = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": test_query}]}\n",
    ")\n",
    "\n",
    "for el in result[\"messages\"]:\n",
    "    print(f\"{el.type}: {el.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162d2c6-2d62-4dc5-b384-652d858dd3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
