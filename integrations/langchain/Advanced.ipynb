{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c05ad14",
   "metadata": {},
   "source": [
    "# Using LangChain Middleware with ChatWriter\n",
    "[Middleware](https://docs.langchain.com/oss/python/langchain/middleware/overview) is a powerful feature in LangChain that lets you **control and customize the behavior of a ChatWriter agent at every step** of its execution.\n",
    "\n",
    "This guide demonstrates how to apply both **built-in and custom middleware** to an agent. You'll explore features such as PII detection, call limits, human-in-the-loop workflows, prompt transformation, and more. By following this guide, you'll learn how to **monitor agent behavior, modify tool usage, and integrate advanced logic** into your agent workflow.\n",
    "\n",
    "## Prerequisites\n",
    "Before getting started, you'll need:\n",
    "- A Python environment with **LangChain** installed\n",
    "- Access to a **ChatWriter-compatible model**\n",
    "- Any tools you plan to integrate with your agent\n",
    "\n",
    "## Setup\n",
    "Install LangChain if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-writer langchain langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83228aad-b12a-443b-aa39-186ffaef9bbb",
   "metadata": {},
   "source": [
    "Next, set the `WRITER_API_KEY` environment variable. We recommend setting it in a `.env` file in the root of your project, but this tutorial will set it in an environment variable if you don't have a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e069fa0-c83b-450d-a27a-59133939c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from writerai import Writer\n",
    "\n",
    "if not os.getenv(\"WRITER_API_KEY\"):\n",
    "    os.environ[\"WRITER_API_KEY\"] = getpass.getpass(\"Enter your Writer API key: \")\n",
    "\n",
    "chat = ChatWriter(model=\"palmyra-x5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c6fa5",
   "metadata": {},
   "source": [
    "## âœ… Example: Agent with Built-in Middleware\n",
    "\n",
    "List of all built-in [middlewares](https://docs.langchain.com/oss/python/langchain/middleware/built-in).\n",
    "\n",
    "We create an agent that uses ChatWriter and attach middleware that detects PII in inputs/outputs (via `PIIMiddleware`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac8dde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_writer import ChatWriter\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "\n",
    "# Create agent with ChatWriter and PII detection middleware\n",
    "agent = create_agent(\n",
    "    model=chat,  # chat model instance\n",
    "    middleware=[\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True)  # redact emails in input\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffea467b-cf01-4af4-af45-ceb016876a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract the email from the next message: 'Check if the email [REDACTED_EMAIL] exists'\n",
      "I'm not going to extract the email from your message because it appears you have already redacted it. If you need help with verifying an email address, I can provide general guidance on how to do so.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"extract the email from the next message: 'Check if the email example@gmail.com exists'\"}]})\n",
    "for el in result[\"messages\"]:\n",
    "    print(el.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095bb4b",
   "metadata": {},
   "source": [
    "## ðŸ›  Custom Middleware Example (Logging / Debugging Hook)\n",
    "\n",
    "You can define your own middleware by subclassing or using hooks to run custom logic.  \n",
    "More info about decorators [here](https://docs.langchain.com/oss/python/langchain/middleware/custom#decorator-based-middleware).  \n",
    "\n",
    "This is useful for logging, analytics, custom validations, formatting, and other custom behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abff5ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context Middleware: messages=2\n",
      " State Context: Long conversation detected\n",
      "Give me exampples for the most powerfull quantcopmucters\n",
      "Give me exampples for the most powerfull quantcopmucters\n",
      "I think you meant \"quantum computers.\" Here are some examples of the most powerful quantum computers:\n",
      "\n",
      "1. **IBM Quantum System One**: A 53-qubit quantum computer that is commercially available.\n",
      "2. **Google Sycamore**: A 53-qubit quantum processor that demonstrated quantum supremacy in 2019.\n",
      "3. **Rigetti Computing's Aspen-11**: A 32-qubit quantum computer that is available for cloud access.\n",
      "4. **IonQ's Harmony**: A 32-qubit trapped-ion quantum computer that is highly accurate.\n",
      "5. **Quantum Circuits Inc.'s (QCI) Quantum Computer**: A 32-qubit quantum computer that uses superconducting qubits.\n",
      "\n",
      "These quantum computers are pushing the boundaries of what's possible in computing and are being used for research and development in various fields.\n",
      "\n",
      "Would you like to know more about a specific type or application of quantum computers?\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import ModelRequest, dynamic_prompt\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def context_aware_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Middleware that handles conversation context.\n",
    "    \"\"\"\n",
    "    message_count = len(request.messages)\n",
    "    print(f\"Context Middleware: messages={message_count}\")\n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "    if message_count >= 2:\n",
    "        prompt_parts.append(\"This is a long conversation - be extra concise.\")\n",
    "        print(\" State Context: Long conversation detected\")\n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    middleware=[context_aware_prompt]\n",
    ")\n",
    "\n",
    "\n",
    "res2 = agent.invoke({\"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Give me exampples for the most powerfull quantcopmucters\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me exampples for the most powerfull quantcopmucters\"}\n",
    "]})\n",
    "for el in res2[\"messages\"]:\n",
    "    print(el.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e736d34-abe9-4229-93b0-353d8e44617b",
   "metadata": {},
   "source": [
    "# Context Engineering in Agents\n",
    "\n",
    "## Overview\n",
    "\n",
    "The hardest part of building reliable agents (or any LLM-powered application) is not just picking a strong model â€” itâ€™s ensuring the right context is passed to that model for each call. When agents fail in real-world use, it is often because the context was wrong, incomplete, or poorly formatted.\n",
    "\n",
    "**Context engineering** means deliberately constructing and managing what context (information, state, memory, configuration) is made available to the LLM â€” and how â€” so that tasks succeed reliably.  \n",
    "\n",
    "With LangChain + LangGraph, context is first-class: you get **runtime context**, **state**, and **persistent store**, with full control over how these feed into model calls, tools, prompts, and middleware.  \n",
    "More info [here](https://docs.langchain.com/oss/python/concepts/context).\n",
    "\n",
    "Now, let's see how static runtime context works â€” passing fixed configuration (like role, style, or preferences) to the agent so it influences every model call without changing during execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9eb0b52a-f157-43d9-a71e-44963486361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Final prompt: You are a helpful assistant.\n",
      "Provide detailed, step-by-step breakdowns and rationales.***\n",
      "Analyze Tesla stock performance in 2024 and explain the key drivers step by step....\n",
      "Tesla's stock performance in 2024 was quite eventful. The stock price fluctuated significantly throughout the year. At the beginning of 2024, the stock price was around $248.42. It reached an all-time high closing price of $479.86 on December 17, 2024. The stock price closed at $403.84 on December 3...\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "# Define context schema for agent\n",
    "@dataclass\n",
    "class ContextSchema:\n",
    "    communication_style: str  # e.g., \"Junior Analyst\", \"Portfolio Manager\"\n",
    "\n",
    "# Dynamic prompt middleware that adapts to user communication style\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:  \n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "    # Get communication style from runtime context\n",
    "    communication_style = request.runtime.context.communication_style\n",
    "\n",
    "    if communication_style == \"Junior Analyst\":\n",
    "        prompt_parts.append(\n",
    "            \"Provide detailed, step-by-step breakdowns and rationales.\"\n",
    "        )\n",
    "    elif communication_style == \"Portfolio Manager\":\n",
    "        prompt_parts.append(\n",
    "            \"Provide concise summaries emphasizing decision-critical metrics and outcomes.\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_parts.append(\"Provide balanced responses.\")\n",
    "    \n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    print(f\"***Final prompt: {final_prompt}***\")\n",
    "    return final_prompt\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"web_search\",\n",
    "    \"function\": {}\n",
    "}]\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=tools,\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=ContextSchema,\n",
    ")\n",
    "\n",
    "res3 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze Tesla stock performance in 2024 and explain the key drivers step by step.\"}]},\n",
    "    context=ContextSchema(communication_style=\"Junior Analyst\")  \n",
    ")\n",
    "for el in res3[\"messages\"]:\n",
    "    print(f\"{el.content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a7949-953c-4484-9297-d374a4cce8b1",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Example: Context-Aware Prompt Using Persistent Store and Runtime Context\n",
    "\n",
    "This example shows how to personalize agent behavior using **user-specific preferences** stored in a **persistent store**, based on a unique `user_id`.\n",
    "\n",
    "The `personalized_prompt` middleware:\n",
    "- Accesses `user_id` from the runtime context  \n",
    "- Retrieves user preferences (e.g., communication style) from the store  \n",
    "- Dynamically adjusts the system prompt based on stored preferences  \n",
    "- Falls back to a balanced style if no valid preference is found  \n",
    "\n",
    "This enables:\n",
    " - Personalized responses per user  \n",
    " - Persistent preference storage across sessions  \n",
    " - Automatic adaptation of prompt behavior using runtime context  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc4e24cd-17dc-4fbd-b760-bae641647b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserState:\n",
    "    user_id: str\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"\n",
    "    Middleware to generate prompts based on stored user preferences.\n",
    "    \"\"\"\n",
    "    store = request.runtime.store  # Access persistent storage\n",
    "    user_id = str(request.runtime.context.user_id)  # Get current user ID\n",
    "    user_prefs = store.get((\"preferences\",), user_id)  # Fetch preferences for user\n",
    "\n",
    "    prompt_parts = [\"You are a helpful assistant.\"]\n",
    "\n",
    "    if not user_prefs or not user_prefs.value.get(\"communication_style\"):\n",
    "        prompt_parts.append(\n",
    "            \"Provide balanced responses\"\n",
    "        )\n",
    "    elif user_prefs.value.get(\"communication_style\") == \"Junior Analyst\":\n",
    "         prompt_parts.append(\n",
    "                \"Provide detailed, step-by-step breakdowns and rationales for all responses.\"\n",
    "            )\n",
    "    elif user_prefs.value.get(\"communication_style\") == \"Portfolio Manager\":\n",
    "        prompt_parts.append(\n",
    "                \"Provide concise summaries emphasizing decision-critical metrics and outcomes.\"\n",
    "            )\n",
    "    else:\n",
    "        prefs = user_prefs.value.get(\"communication_style\")\n",
    "        print(f\"Found unexpected preference - {prefs}\")\n",
    "        prompt_parts.append(\n",
    "            \"Provide balanced responses\"\n",
    "        )\n",
    "       \n",
    "    final_prompt = \"\\n\".join(prompt_parts)\n",
    "    print(f\"***Final prompt: {final_prompt}***\")\n",
    "    return final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e8e84-b9d4-46b3-b431-71026428623e",
   "metadata": {},
   "source": [
    "### Description â€” Agent with persistent per-user preferences\n",
    "\n",
    "This code creates a LangChain agent that uses a persistent in-memory store to apply **per-user prompt personalization** at runtime.\n",
    "\n",
    "**Key behaviors**\n",
    "- The `personalized_prompt` middleware reads `user_id` from `context` and looks up that userâ€™s preferences in the store to adapt the system prompt (e.g., detailed step-by-step vs. concise summary).\n",
    "- Using `InMemoryStore` keeps preferences in-process for this example; swap for a persistent store (Redis/DB) in production.\n",
    "\n",
    "**Notes**\n",
    "- `user_id` used when calling `agent.invoke(..., context=UserState(user_id=user_id))` controls which stored preferences apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c6ee71f-f19c-4b78-96d0-3ca74f4cb89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Final prompt: You are a helpful assistant.\n",
      "Provide detailed, step-by-step breakdowns and rationales for all responses.***\n",
      "Analyze Tesla stock performance in 2024 and explain the key drivers step by step....\n",
      "Tesla's stock performance in 2024 was quite eventful. The stock price fluctuated significantly throughout the year. At the beginning of 2024, the stock price was around $248.42. It reached an all-time high closing price of $479.86 on December 17, 2024. The stock price closed at $403.84 on December 3...\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=chat,\n",
    "    tools=tools,\n",
    "    middleware=[personalized_prompt],\n",
    "    context_schema=UserState,\n",
    "    store=InMemoryStore()\n",
    ")\n",
    "\n",
    "# Reference to the agent's store\n",
    "store = agent.store\n",
    "\n",
    "# Example user IDs\n",
    "user_id = 1\n",
    "\n",
    "# Store user preferences in the persistent store\n",
    "store.put((\"preferences\",), user_id, {\"communication_style\": \"Junior Analyst\"})\n",
    "store.put((\"preferences\",), 2, {\"communication_style\": \"Portfolio Manager\"})\n",
    "\n",
    "\n",
    "# Invoke the agent for a specific user with runtime context\n",
    "res3 = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze Tesla stock performance in 2024 and explain the key drivers step by step.\"}]},\n",
    "    context=UserState(user_id=user_id)  # runtime context determines which preferences apply \n",
    ")\n",
    "\n",
    "\n",
    "for el in res3[\"messages\"]:\n",
    "    print(f\"{el.content[:300]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
