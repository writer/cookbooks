{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477eaccc-6313-4da7-a6fb-5e03b55bf427",
   "metadata": {},
   "source": [
    "# Integration with LangChain\n",
    "\n",
    "This notebook demonstrates how to use the WRITER ecosystem within LangChain, including setup, chat model usage, and tool invocation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before getting started, ensure the following prerequisites are met:\n",
    "\n",
    "- A [Writer AI Studio](https://app.writer.com/register) account\n",
    "- An API key, which you can obtain by following the [API Quickstart](https://dev.writer.com/api-guides/quickstart)\n",
    "\n",
    "\n",
    "## üì¶ Installation\n",
    "Install the Writer integration package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e59366-9496-48aa-9376-68b723beef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-writer langchain langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b629a7-35f6-4b4a-a057-3fc7b387ac3b",
   "metadata": {},
   "source": [
    "## üîë Setup API Key\n",
    "\n",
    "Next, set the `WRITER_API_KEY` environment variable. Setting it in a `.env` file in the root of the project is recommended; however, this tutorial sets it directly as an environment variable if a `.env` file is not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70145ade-2b7d-4072-bf4e-0670dd5d2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.getenv(\"WRITER_API_KEY\"):\n",
    "    os.environ[\"WRITER_API_KEY\"] = getpass.getpass(\"Enter your Writer API Key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec39b2e-8026-4541-9ee3-e8400aa864c0",
   "metadata": {},
   "source": [
    "## Initialize a Writer chat model with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4345248-fed8-4347-b525-84149c3a6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_writer import ChatWriter\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "model = ChatWriter(model=\"palmyra-x5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a156de6-a331-4ee5-9e2c-410535c0ac6e",
   "metadata": {},
   "source": [
    "## üß† Chat model usage\n",
    "\n",
    "The package is now installed and the WRITER API key is configured.\n",
    "LangChain can now be used to create a chat with a Palmyra model.\n",
    "\n",
    "The `model` argument in `ChatWriter` specifies which Palmyra model to use, such as `palmyra-x5`, `palmyra-general`, or domain-specific models. You can send structured messages using `SystemMessage` and `HumanMessage`.\n",
    "\n",
    "> **Best practice:** Use a `SystemMessage` to define the assistant‚Äôs role, tone, and output structure. This helps generate more consistent and reliable results, especially when expecting structured JSON outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f83a3-8aac-45a7-9bf7-d75734d23b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_prompt = \"Generate a blog outline for AI in healthcare.\"\n",
    "\n",
    "response = model.invoke([\n",
    "    SystemMessage(content=\"You are a blog planning assistant. Generate a structured blog plan based on the user's requirements. Respond with a JSON object containing a 'sections' array where each section has 'name', 'description', and 'main_body' fields.\"),\n",
    "    HumanMessage(content=sections_prompt)\n",
    "])\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e8dac-28db-4437-9fdc-0fc1a04916ba",
   "metadata": {},
   "source": [
    "## üöÄ LangGraph integration with ChatWriter\n",
    "\n",
    "This section demonstrates how to build a LangGraph agent using the `ChatWriter` model. The process includes defining tools, the agent state, and constructing the execution graph in a cookbook-friendly style.\n",
    "\n",
    "### üì¶ Step 1: Imports\n",
    "\n",
    "Import LangChain and LangGraph essentials, including message types, tool decorators, and the graph framework. `Annotated` is used to manage state updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859f69a-deeb-4cd5-831f-0a4836c745d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_writer import ChatWriter\n",
    "from langchain.tools import tool\n",
    "from langchain.messages import SystemMessage, HumanMessage, ToolMessage, AnyMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777fcbce-040d-4eea-8d26-88cc3cc48d5d",
   "metadata": {},
   "source": [
    "### üõ† Step 2: Define Tools\n",
    "\n",
    "This step defines a few basic arithmetic tools. In practice, any callable can be defined as a tool. Using the `@tool` decorator allows LangGraph to recognize these as callable nodes in the execution graph.\n",
    "\n",
    "> Tip: Keep tool interfaces simple and clearly typed to ensure the agent can call them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f5b60-fac7-49d1-90da-cf812df6dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Return the sum of two integers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Return the product of two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Return the result of dividing the first integer by the second.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model = ChatWriter(model=\"palmyra-x5\")\n",
    "model_with_tools = model.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26807c-2d64-4d46-8907-2a6fa0fdef9f",
   "metadata": {},
   "source": [
    "### üìä Step 3: Define Agent State\n",
    "\n",
    "LangGraph uses a structured `state` object to keep track of messages and other context. Using `Annotated` with `operator.add` ensures that new messages are appended rather than overwriting existing ones.\n",
    "\n",
    "> Tip: You can extend the state with custom fields if your agent needs additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e210955-27b6-453a-9733-446cd505ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c74a5-7f3c-4ed3-a659-4f15760f924f",
   "metadata": {},
   "source": [
    "### üí¨ Step 4: Define LLM Node\n",
    "\n",
    "This node represents the LLM call. It decides if tools should be invoked based on the conversation context. The returned dictionary updates the agent state.\n",
    "\n",
    "> Tip: Keep system instructions clear to guide the LLM on how to handle tool calls and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45539b70-d1cb-4a51-9bce-23b0aa26ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: dict):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            model_with_tools.invoke([\n",
    "                SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "            ] + state[\"messages\"])\n",
    "        ],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e0d23-32c6-4b31-8faf-809dd803c35e",
   "metadata": {},
   "source": [
    "### üõ† Step 5: Define Tool Node\n",
    "\n",
    "The tool node executes any tool calls made by the LLM. Each tool call produces a `ToolMessage` which is appended to the agent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efad959-e417-424f-a9cf-91a8b8ce3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_node(state: dict):\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfeb2bb-f298-4d33-ada2-a98df1fbf77d",
   "metadata": {},
   "source": [
    "### üîÅ Step 6: Define Conditional Edge\n",
    "\n",
    "This function controls the flow of the graph. If the LLM has requested a tool call, the agent proceeds to the tool node; otherwise, it ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf80e7-50b2-4613-a19c-bf1467a8ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2b29c-58ca-4103-b441-ac7403735e37",
   "metadata": {},
   "source": [
    "### üèó Step 7: Build and Compile Agent\n",
    "\n",
    "Assemble the graph, add nodes, define edges, and compile the agent.\n",
    "\n",
    "> Tip: You can visualize this graph before compilation to debug the flow of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8545bd6-6877-4b97-a708-b1279073b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_builder = StateGraph(MessagesState)\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\"llm_call\", should_continue, [\"tool_node\", END])\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321afc8-07b8-495d-82a5-cfa4dff9dfad",
   "metadata": {},
   "source": [
    "### üñº Step 8: Visualize\n",
    "\n",
    "Use the built-in graph visualization to see the execution flow. This is especially helpful for complex agents with many nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a4e92-cc8d-40db-bcd8-6b63d0fa9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea6d55-3dd8-4c7c-8ccf-42796b66e37f",
   "metadata": {},
   "source": [
    "### Step 9: Test Agent\n",
    "\n",
    "Send a test message to your agent and print the results. Each message returned can be pretty-printed for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = agent.invoke({\"messages\": messages}, {\"recursion_limit\": 7})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# messages = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Add 3 and 4.\"}]}, {\"recursion_limit\": 7})\n",
    "# for m in messages[\"messages\"]:\n",
    "#     m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# DeepAgents with ChatWriter and web search\n",
    "\n",
    "This notebook demonstrates how to build a deep agent using `ChatWriter` and Writer's Web Search tool to perform real-time research and generate reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "### üì¶ Step 1: Imports\n",
    "\n",
    "Import `ChatWriter`, `create_deep_agent`, and message types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c29a62-30ae-45bb-bc4e-10321985e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install deepagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_writer import ChatWriter\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from deepagents import create_deep_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_setup",
   "metadata": {},
   "source": [
    "### üõ† Step 2: initialize the model and Writer client\n",
    "\n",
    "Assumes `WRITER_API_KEY` is already set in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "websearch_tool",
   "metadata": {},
   "source": [
    "### üîç Step 3: define web search tool\n",
    "\n",
    "The tool configuration tells the agent how to use the Writer Web Search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "websearch_tool_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"web_search\",\n",
    "    \"function\": {\n",
    "        \"include_domains\": [\"www.ibm.com/quantum\"],  # Optional: restrict to certain domains\n",
    "        # \"exclude_domains\": []   # Optional: exclude domains\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create_agent",
   "metadata": {},
   "source": [
    "### ü§ñ Step 4: create deep agent\n",
    "\n",
    "Use `create_deep_agent` with the ChatWriter model and the Web Search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_agent_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are an expert researcher. Your job is to answer questions accurately and provide references.\n",
    "You have access to the Web Search tool to retrieve up-to-date information from the internet.\n",
    "'''\n",
    "\n",
    "agent = create_deep_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_agent",
   "metadata": {},
   "source": [
    "### üèÉ Step 5: run the agent with a query\n",
    "\n",
    "Send a research question to the agent and get a response using Web Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_agent_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Find recent breakthroughs in quantum computing. Include references from at most 3 sources.\"}]})\n",
    "print(result['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
