{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming with tool calling\n",
    "Using tool calling with streaming responses requires a different approach than when using non-streaming chat completions. Let's walk through an example of how to do this.\n",
    "\n",
    "To get started, first install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install writer-sdk python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "The cell below performs the initialization required for this notebook including the creation of an instance of the `Writer` object to interact with the LLM.\n",
    "\n",
    "To create a Writer client object, you need an API key. [You can sign up for one for free](https://app.writer.com/register). \n",
    "\n",
    "Once you have an API key, we recommend that you store it as an environment variable in a `.env` file like so:\n",
    "\n",
    "```\n",
    "WRITER_API_KEY=\"{Your Writer API key goes here}\"\n",
    "```\n",
    "\n",
    "When you instantiate the client with `client = Writer()`, the newly-created object will automatically look for an environment variable named `WRITER_API_KEY` and will complete the instantiation if an only if `WRITER_API_KEY` has been defined. This notebook uses the [python-dotenv] library to automatically define environment variables based on the contents of an `.env` file in the same directory.\n",
    "\n",
    "The `Writer()` initializer method also has an `api_key` parameter that you can use like this...\n",
    "\n",
    "```\n",
    "client = Writer(api_key=\"{Your Writer API key goes here}\")\n",
    "```\n",
    "\n",
    "...but we strongly encourage you not to leave API keys in your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from writerai import Writer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "client = Writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a tool\n",
    "\n",
    "As with non-streaming tool calling, we need to define the tools that our model can use. In this example, we'll define a tool that calculates the mean of a list of numbers, as well as a JSON schema to be passed to the model. You can feel free to define any tools you'd like here, such as tools to perform database queries or interact with other external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(numbers: list) -> float:\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "tools = [\n",
    "    { \n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_mean\", \n",
    "            \"description\": \"Calculate the mean (average) of a list of numbers.\", \n",
    "            \"parameters\": { \n",
    "                \"type\": \"object\", \n",
    "                \"properties\": { \n",
    "                    \"numbers\": { \n",
    "                        \"type\": \"array\", \n",
    "                        \"items\": {\"type\": \"number\"}, \n",
    "                        \"description\": \"List of numbers\"\n",
    "                    } \n",
    "                }, \n",
    "                \"required\": [\"numbers\"] \n",
    "            } \n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the model\n",
    "\n",
    "Now that we've defined our tool and schema, we can call the model. Set `stream=True` to get a streaming response and pass the `tools` and `tool_choice` parameters for tool calling. Use `tool_choice=\"auto\"` to let the model decide when to use the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"what is the mean of [1,3,5,7,9]?\"}]\n",
    "\n",
    "response = client.chat.chat(\n",
    "    model=\"palmyra-x-004\", \n",
    "    messages=messages, \n",
    "    tools=tools, \n",
    "    tool_choice=\"auto\", \n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the response\n",
    "\n",
    "Processing the streaming response requires a different approach than when using non-streaming chat completions.\n",
    "\n",
    "Here's a high-level overview of what the code below is doing:\n",
    "\n",
    "**1. Initialization and iterating over the response**\n",
    "- `streaming_content` is initialized as an empty string to accumulate content.\n",
    "- `function_calls` is initialized as an empty list to store tool call information.\n",
    "- The code iterates over response, which is expected to be a streaming response from a chat model.\n",
    "- For each chunk, the first choice is accessed via `chunk.choices[0]`.\n",
    "\n",
    "**2. Handling tool call and non-tool-call content**\n",
    "- If `choice.delta.tool_calls` is not `null`, it indicates that the model is suggesting tool calls. Each chunk will contain bits and pieces of the tool calls throughout the streaming response, so we need to collect them and store them in `function_calls`.\n",
    "  - For each `tool_call`, if an ID is present, a dictionary with the tool call ID is appended to `function_calls`.\n",
    "  - If a function is specified, its name and arguments are appended to the last dictionary in `function_calls`.\n",
    "- If `choice.delta.tool_calls` is `null` but `choice.delta.content` exists, it indicates that no tool calls are being made and the content is appended to `streaming_content`.\n",
    "\n",
    "**3. Finish reasons**\n",
    "- If `choice.finish_reason` is `stop`, it means the model has finished generating the response (and thus has made no further tool calls), and the accumulated `streaming_content` is appended to `messages` with the role `assistant`.\n",
    "- If `choice.finish_reason` is `tool_calls`, it indicates the model has finished deciding which tools to call and we can now execute the tool calls.\n",
    "\n",
    "**4. Executing tool calls**\n",
    "- For each `function_call` in `function_calls`, if the function name is \"calculate_mean\", the arguments are parsed from JSON.\n",
    "- The `calculate_mean` function is called with the parsed arguments, and the result is appended to `messages` with the role `tool`.\n",
    "\n",
    "**5. Final response**\n",
    "- A new chat request is made with the updated messages to get the final response.\n",
    "- To demonstrate the streaming response, `print(choice.delta.content, end=\"\", flush=True)` is used to print the content as it comes in.\n",
    "- The streaming content is also accumulated in `final_streaming_content` and then printed when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_content = \"\"\n",
    "function_calls = []\n",
    "\n",
    "for chunk in response:\n",
    "    choice = chunk.choices[0]\n",
    "\n",
    "    if choice.delta:\n",
    "        # Check for tool calls\n",
    "        if choice.delta.tool_calls:\n",
    "            for tool_call in choice.delta.tool_calls:\n",
    "                if tool_call.id:\n",
    "                    # Append an empty dictionary to the function_calls list with the tool call ID\n",
    "                    function_calls.append(\n",
    "                        {\"name\": \"\", \"arguments\": \"\", \"call_id\": tool_call.id}\n",
    "                    )\n",
    "                if tool_call.function:\n",
    "                    # Append function name and arguments to the last dictionary in the function_calls list\n",
    "                    function_calls[-1][\"name\"] += (\n",
    "                        tool_call.function.name\n",
    "                        if tool_call.function.name\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    function_calls[-1][\"arguments\"] += (\n",
    "                        tool_call.function.arguments\n",
    "                        if tool_call.function.arguments\n",
    "                        else \"\"\n",
    "                    )\n",
    "        # Handle non-tool-call content\n",
    "        elif choice.delta.content:\n",
    "            streaming_content += choice.delta.content\n",
    "            print(choice.delta.content, end=\"\", flush=True)\n",
    "\n",
    "        # A finish reason of stop means the model has finished generating the response\n",
    "        if choice.finish_reason == \"stop\":\n",
    "            messages.append({\"role\": \"assistant\", \"content\": streaming_content})\n",
    "\n",
    "        # A finish reason of tool_calls means the model has finished deciding which tools to call\n",
    "        elif choice.finish_reason == \"tool_calls\":\n",
    "            for function_call in function_calls:\n",
    "                if function_call[\"name\"] == \"calculate_mean\":\n",
    "                    arguments_dict = json.loads(function_call[\"arguments\"])\n",
    "                    function_response = calculate_mean(arguments_dict[\"numbers\"])\n",
    "\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"tool\",\n",
    "                            \"content\": str(function_response),\n",
    "                            \"tool_call_id\": function_call[\"call_id\"],\n",
    "                            \"name\": function_call[\"name\"],\n",
    "                        }\n",
    "                    )\n",
    "               \n",
    "                final_response = client.chat.chat(\n",
    "                    model=\"palmyra-x-004\", messages=messages, stream=True\n",
    "                )\n",
    "\n",
    "                final_streaming_content = \"\"\n",
    "                for chunk in final_response:\n",
    "                    choice = chunk.choices[0]\n",
    "                    if choice.delta and choice.delta.content:\n",
    "                        final_streaming_content += choice.delta.content\n",
    "                        print(choice.delta.content, end=\"\", flush=True)\n",
    "                print(f\"\\n\\nCompleted response: {final_streaming_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we've now seen how to use tool calling with streaming responses. To learn more about tool calling, check out the [tool calling guide](https://dev.writer.com/api-guides/tool-calling) on the Writer docs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
