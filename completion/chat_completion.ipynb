{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cf920a-8224-40a0-bc53-068c701c571c",
   "metadata": {},
   "source": [
    "# **Chat completions with Writer**\n",
    "\n",
    "**_Chat completions_** are LLM responses to the user in a conversation or chat. They tend to be shorter than text completions, but are often part of a sequence of exchanges between the user and the model. This cookbook shows how to use the `chat()` method of Writer’s `chat` object to build apps for single-turn and multi-turn chats, with streaming and non-streaming responses.\n",
    "\n",
    "## **Contents**\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Setup](#setup)\n",
    "- [The `chat` object](#the-chat-object)\n",
    "- [Single-turn chat completion (non-streaming version)](#single-turn-chat-completion-non-streaming-version)\n",
    "- [Multi-turn chat completion (non-streaming version)](#multi-turn-chat-completion-non-streaming-version)\n",
    "- [Single-turn chat completion (streaming version)](#single-turn-chat-completion-streaming-version)\n",
    "- [Multi-turn chat completion (streaming version)](#multi-turn-chat-completion-streaming-version)\n",
    "- [For more information](#for-more-information)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "## **Introduction**\n",
    "\n",
    "### What are chat completions?\n",
    "\n",
    "**In chat completion, the model interacts with the user in a conversational format, _completing_ a chat by responding to them.** While a chat system can be used to get a single response to a single input, chat completion systems are designed to participate in a back-and-forth conversation with the user, which requires it to retain the history of a conversation and derive context from that history to generate useful and meaningful responses as the conversation continues.\n",
    "\n",
    "Chat completion, as its name implies, is meant as a way to complete a conversation by providing the user with a human-like conversation partner. The term _completion_ is also used to mean the answer that a model provides in response to a user prompt.\n",
    "\n",
    "There are two general categories of chat completion:\n",
    "\n",
    "1. **“Single-turn” completions,** where the model is given a single prompt and generates a response or solution in a single step, without any follow-up. You may want to think of “single-turn” chat completions as shorter-form versions of text completions.\n",
    "2. **“Multi-turn” completions,** where the model and user have a back-and-forth conversation with multiple exchanges. In this kind of completion, the model appears to maintain a “sense of context” because it “remembers” what took place earlier in the conversation.\n",
    "\n",
    "For each of these categories, there are two options for the way the completion is returned:\n",
    "\n",
    "1. **Non-streaming:** The model does not return a completion until it has been completely generated. There is a waiting period (typically a few seconds) while the completion is being generated.\n",
    "2. **Streaming:** The model returns the completion in chunks as it’s being generated. You need to write additional code to collect and assemble these chunks, but there’s almost no waiting period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a6818-1a5a-497d-b392-704276d98148",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## **Setup**\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook uses the following packages:\n",
    "\n",
    "* `ipywidgets`: To draw UI widgets for the streaming versions of the apps.\n",
    "* `python-dotenv`: To load environment variables.\n",
    "* `writer-sdk`: To access the Writer API.\n",
    "\n",
    "Run the cell below ensure you have these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be663d2-6860-42f8-a4b6-37aeb269d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a622c-07aa-4690-9a9a-dd709b259b5e",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "The cell below performs the initialization required for this notebook including the creation of an instance of the `Writer` object to interact with the LLM.\n",
    "\n",
    "To create a Writer client object, you need an API key. [You can sign up for one for free](https://app.writer.com/aistudio/signup). \n",
    "\n",
    "Once you have an API key, we recommend that you store it as an environment variable in a `.env` file like so:\n",
    "\n",
    "```\n",
    "WRITER_API_KEY=\"{Your Writer API key goes here}\"\n",
    "```\n",
    "\n",
    "When you instantiate the client with `client = Writer()`, the newly-created object will automatically look for an environment variable named `WRITER_API_KEY` and will complete the instantiation if an only if `WRITER_API_KEY` has been defined. This notebook uses the [python-dotenv](https://pypi.org/project/python-dotenv/) library to automatically define environment variables based on the contents of an `.env` file in the same directory.\n",
    "\n",
    "The `Writer()` initializer method also has an `api_key` parameter that you can use like this...\n",
    "\n",
    "```\n",
    "client = Writer(api_key=\"{Your Writer API key goes here}\")\n",
    "```\n",
    "\n",
    "...but we strongly encourage you not to leave API keys in your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980e957-6193-4086-8ecd-ce921ca3eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before running any other cells in this cookbook!\n",
    "\n",
    "from writerai import Writer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "client = Writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742c81b-a698-4703-8d48-c3d0751e02ce",
   "metadata": {},
   "source": [
    "<a id=\"the-chat-object\"></a>\n",
    "## **The `chat` object**\n",
    "\n",
    "Now that you have a Writer client instance, it’s time to start building chat completion apps! \n",
    "\n",
    "The `chat` property of a Writer client instance contains methods and properties related to chat completion. In all the examples in this cookbook, you’ll build chat completion apps by using the `chat` property’s `chat()` method, which makes requests for chat completions from Palmyra, Writer's custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1dfcaa-157c-4c92-80dd-055aedbf8af8",
   "metadata": {},
   "source": [
    "<a id=\"single-turn-chat-completion-non-streaming-version\"></a>\n",
    "## **Single-turn chat completion (non-streaming version)**\n",
    "\n",
    "The cell below contains a simple single-turn chat completion app. When you run it, you will be asked to enter a prompt. After you enter the prompt, you can expect to wait a moment or two while Palmyra generates the complete text of its response. Once Palmyra’s done generating, the app will display the response and finish running.\n",
    "\n",
    "Try entering a simple question or command that can be answered or satisfied with just one reply (e.g. “What’s the fastest land animal?” or “I need some synonyms for ‘awesome’”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a18c59-0c5f-4611-b3da-39bbc565cf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. Respond concisely and politely to user queries. Use clear, simple language. When asked for technical explanations, provide detailed and accurate information, but avoid jargon. If the user asks for assistance with a task, offer step-by-step guidance.\"\n",
    "}\n",
    "messages = [initial_system_message]\n",
    "\n",
    "print(\"\"\"\n",
    "Sample single-turn chat completion app\n",
    "======================================\n",
    "\"\"\")\n",
    "\n",
    "user_prompt = input(\"Enter a prompt: \").strip()\n",
    "user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": user_prompt\n",
    "}\n",
    "temperature = float(input(\"Enter a temperature (0.0 - 2.0), or just press 'Enter' for 1.0: \").strip() or 1.0)\n",
    "messages.append(user_message)\n",
    "chat_response = client.chat.chat(\n",
    "    messages=messages,\n",
    "    temperature=temperature,\n",
    "    model=\"palmyra-x-004\",\n",
    "    stream=False\n",
    ")\n",
    "print(f\"\\n{chat_response.choices[0].message.content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d435d0e-250b-4880-8461-a672f018454f",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "#### Messages\n",
    "\n",
    "LLMs with conversational interfaces like chat completions keep track of conversations using a list of messages categorized by roles. With Palmyra, there are three roles:\n",
    "\n",
    "1. **`user`**: This role is for messages containing user input or prompts, which Palmyra responds to. From the user’s point of view, `user` messages drive the conversation. An example of a `user` message is “What’s the fastest land animal?”\n",
    "2. **`assistant`**: This role is for messages containing Palmyra’s responses to the user’s input. Palmyra is playing the role of AI assistant. An example of an `assistant` message is “The fastest land animal is the cheetah. It can reach speeds up to 70-75 mph (112-120 km/h) in short bursts.”\n",
    "3. **`system`**: This role is for messages that function as additional instructions or that define how the assistant should behave during the conversation. A `system` message guides the behavior or personality of the assistant, specifying how Palmyra should respond, and is typically set at the start of a conversation. An example of a `system` message is “You are a knowledgeable assistant that provides concise answers to technical questions.”\n",
    "\n",
    "#### Calling the `chat()` method\n",
    "\n",
    "The heart of the single-turn chat completion app is this line:\n",
    "\n",
    "```python\n",
    "chat_response = client.chat.chat(\n",
    "    messages=messages,\n",
    "    temperature=temperature,\n",
    "    model=\"palmyra-x-004\",\n",
    "    stream=False\n",
    ")\n",
    "```\n",
    "\n",
    "It makes a call to the client instance’s `chat` object’s `chat()` method, which requests a chat completion from Palmyra. In order to get that completion, the call provides arguments for the following parameters:\n",
    "\n",
    "<table width=\"66%\">\n",
    "    <tr>\n",
    "        <th width=\"25%\" style=\"background-color: #5551ff; color: #ffffff;\">Parameter</th>\n",
    "        <th style=\"background-color: #5551ff; color: #ffffff;\">Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>messages</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            <p>A list containing message dictionaries that Palmyra will use as a basis for the completion it will return as its response. Message dictionaries have the following keys:</p>\n",
    "            <ul>\n",
    "                <li><code>role</code>: Determines the message type. Valid values are <code>user</code>, <code>assistant</code>, and <code>system</code>. See _Messages_ above for details.</li>\n",
    "                <li><code>content</code>: The actual text of the message.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>temperature</code><br />(optional)</td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            <p>A float that controls the level of randomness in the text that Palmyra generates:</p>\n",
    "            <ul>\n",
    "                <li>The default value is 1.</li>\n",
    "                <li>At temperatures <em>below</em> 1, the responses are more deterministic and predictable, with Palmyra tending to choose the highest probability tokens based on previously-generated ones. The generated output is predictable and repetitive, and produces more \"safe\" or \"obvious\" answers.</li>\n",
    "                <li>At temperatures <em>above</em> 1, the responses are more random and “imaginative,”  with Palmyra giving less probable tokens a better chance of being chosen. The generated output is less predictable, and produces more “creative” answers. The results become increasingly nonsensical at temperatures of about 1.4 and higher, especially for longer completions.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>model</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            A string specifying which model to use. In this case, we’re using the latest model\n",
    "            at the time of writing, <code>palmyra-x-004</code>.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>stream</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            A boolean specifying if the method should stream the completion in chunks\n",
    "            in real time as Palmyra generates it (<code>True</code>) or wait until Palmyra\n",
    "            finishes generating the completion before returning a value (<code>False</code>).\n",
    "            Since we want the completion all at once, we set this parameter to <code>False</code>.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### The response to the `chat()` method\n",
    "\n",
    "When `chat()` is called with its `stream` parameter set to `False`, it returns a `Chat` object with the following properties:\n",
    "\n",
    "<table  width=\"66%\">\n",
    "    <tr>\n",
    "        <th width=\"25%\" style=\"background-color: #5551ff; color: #ffffff;\">Property</th>\n",
    "        <th style=\"background-color: #5551ff; color: #ffffff;\">Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>id</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">A string containing the <code>Chat</code> object’s unique identifier.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>choices</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            <p>A list of <code>Choice</code> objects representing possible completions (usually containing just one). Each <code>Choice</code> object has the following properties:</p>\n",
    "            <ul>\n",
    "                <li><code>finish_reason</code>: The reason Palmyra stopped generating the response. Possible values include <code>\"stop\"</code> for a complete response, and <code>\"length\"</code> if the response was truncated.</li>\n",
    "                <li><code>message</code>: A <code>ChoiceMessage</code> object with two properties, <code>content</code> and <code>role</code>, which serve the same purpose as the <code>content</code> and <code>role</code> keys in a message dictionary.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>created</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">An integer representing the time when the response was created as a Unix timestamp. You can use this to compare the timing of the response with timings of other events.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>model</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">A string specifying the model that generated the response.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Of all the properties listed above, the one we’re most interested in is the `message` property, and within the `ChoiceMessage` object it contains, we’re only interested in its `content` property, which contains the completion that Palmyra generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad2db1-bcf5-4651-9574-ed78182aad7a",
   "metadata": {},
   "source": [
    "<a id=\"multi-turn-chat-completion-non-streaming-version\"></a>\n",
    "## **Multi-turn chat completion (non-streaming version)**\n",
    "\n",
    "The cell below contains a basic multi-turn chat completion app. When you run it, you will be able to have an ongoing conversation with Palmyra until you enter a blank line, which stops the app. The app displays the number of prompts you have entered so far.\n",
    "\n",
    "Like the single-turn app above, when this app requests completions from Palmyra, there’s a pause while it generates the complete text of its response. Once Palmyra’s done generating, the app will display the response and finish running.\n",
    " \n",
    "Unlike the single-turn app, this app not only lets you enter more than one prompt, but also maintains a record of the conversation in the `messages` list — both the user’s messages (the ones where the value of the `\"role\"` key is `\"user\"`), and Palmyra’s replies (messages where the value of the `\"role\"` key is `\"assistant\"`). You can see the contents of `messages` while the app is running by entering `!messages` as a prompt (it will not count as part of the conversation).\n",
    "\n",
    "Try entering a question or command first, and then enter follow-up requests to confirm that the app “remembers” previous parts of the conversation. For example, try “What's the fastest bird?\" for your first prompt, then \"Tell me more\" for your second prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d200f-a797-43de-9609-b9445a44a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_count = 1\n",
    "initial_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. Respond concisely and politely to user queries. Use clear, simple language. When asked for technical explanations, provide detailed and accurate information, but avoid jargon. If the user asks for assistance with a task, offer step-by-step guidance.\"\n",
    "}\n",
    "messages = [initial_system_message]\n",
    "\n",
    "print(\"\"\"\n",
    "Sample multi-turn chat completion app\n",
    "=====================================\n",
    "\"\"\")\n",
    "temperature = float(input(\"Enter a temperature (0.0 - 2.0) for the chat, or just press 'Enter' for 1.0: \").strip() or 1.0)\n",
    "\n",
    "while True:\n",
    "    user_prompt = input(f\"[{user_prompt_count}]\\nEnter a prompt: \").strip()\n",
    "    if not user_prompt:\n",
    "        break\n",
    "\n",
    "    if user_prompt == \"!messages\":\n",
    "        print(f\"\\nContents of `messages` (this will not be included as part of the conversation):\")\n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "        print(f\"{messages}\\n\\n\")\n",
    "        continue\n",
    "\n",
    "    user_prompt_count +=1\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "    \n",
    "    chat_response = client.chat.chat(\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        model=\"palmyra-x-004\",\n",
    "        stream=False\n",
    "    )\n",
    "    chat_response_role = chat_response.choices[0].message.role\n",
    "    chat_response_content = chat_response.choices[0].message.content\n",
    "    print(f\"---\\n{chat_response_content}\\n\\n\")\n",
    "    \n",
    "    response_message = {\n",
    "        \"role\": chat_response_role,\n",
    "        \"content\": chat_response_content\n",
    "    }\n",
    "    messages.append(response_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed46a1-db87-44d3-a470-d64292d885a4",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Aside from running within a loop, the key difference between this version and the single-turn version is that this version keeps track of the conversation by constantly growing the `messages` list. \n",
    "\n",
    "When the user enters a prompt, that prompt gets added to `messages` as a `user` message, and when Palmyra generates a response, it gets added to `messages` as an `assistant` message. Each time the app calls the `chat` method, it provides the latest version of the `messages` list as the argument for the `messages` parameter, giving Palmyra a complete record of the conversation so far, and with it, context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7c0c1-63e0-4d04-9f6c-8a869bb63b21",
   "metadata": {},
   "source": [
    "<a id=\"single-turn-chat-completion-streaming-version\"></a>\n",
    "## **Single-turn chat completion (streaming version)**\n",
    "\n",
    "The cell below is a _streaming_ version of the single-turn chat completion app. With this app, after you enter the prompt, you will immediately see the completion as Palmyra generates it as a stream of text in a manner similar to a lot of AI chat apps.\n",
    "\n",
    "Note that this version of the app uses the [Jupyter Widgets](https://ipywidgets.readthedocs.io/en/stable/) library to provide a graphical user interface for the app. There’s a reason, which will be explained in the notes in the cell after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b8df53-f0fb-4650-a681-641b4eeb0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, ButtonStyle, FloatSlider, Layout, Text, Textarea, VBox\n",
    "\n",
    "def display_ui():\n",
    "    prompt_text_box = Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt here\",\n",
    "        description=\"Prompt:\",\n",
    "        style={\"description_width\": \"100px\"},\n",
    "        layout=Layout(width=\"500px\"),\n",
    "        continuous_update=False,\n",
    "        disabled=False   \n",
    "    )\n",
    "    temperature_slider = FloatSlider(\n",
    "        min=0.0,\n",
    "        max=2.0,\n",
    "        value=1.0,\n",
    "        step=0.1,\n",
    "        orientation=\"horizontal\",\n",
    "        description=\"Temperature:\",\n",
    "        style={\"description_width\": \"100px\"},\n",
    "        layout=Layout(width=\"500px\"),\n",
    "        continuous_update=False,\n",
    "        readout=True,\n",
    "        readout_format=\".1f\",\n",
    "        disabled=False\n",
    "    )\n",
    "    submit_button = Button(\n",
    "        description=\"Submit\",\n",
    "        tooltip=\"Click me\",\n",
    "        style=ButtonStyle(button_color=\"thistle\", font_weight=\"bold\"),\n",
    "        layout = Layout(margin=\"0px 0px 0px 110px\"),\n",
    "        icon=\"upload\",\n",
    "        disabled=False\n",
    "    )\n",
    "    completion_text_area = Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"\",\n",
    "        description=\"Response:\",\n",
    "        layout=Layout(width=\"800px\", height=\"200px\", margin=\"10px 0px 0px 20px\"),\n",
    "        disabled=False\n",
    "    )\n",
    "    display(\n",
    "        VBox(\n",
    "            [\n",
    "                prompt_text_box, \n",
    "                temperature_slider,\n",
    "                submit_button,\n",
    "                completion_text_area,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return (prompt_text_box, temperature_slider, submit_button, completion_text_area)\n",
    "\n",
    "def generate_completion(prompt_text_box, temperature_slider, submit_button, completion_text_area):   \n",
    "    # Define messages\n",
    "    initial_system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. Respond concisely and politely to user queries. Use clear, simple language. When asked for technical explanations, provide detailed and accurate information, but avoid jargon. If the user asks for assistance with a task, offer step-by-step guidance.\"\n",
    "    }\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_text_box.value.strip()\n",
    "    }\n",
    "\n",
    "    # Put UI in \"generating\" mode\n",
    "    prompt_text_box.disabled = True\n",
    "    temperature_slider.disabled = True\n",
    "    submit_button.disabled = True\n",
    "    submit_button.description = \"Generating...\"\n",
    "    submit_button.icon = \"hourglass\"\n",
    "\n",
    "    # Generate completion and display it\n",
    "    chat_response = client.chat.chat(\n",
    "        messages=[initial_system_message, user_message],\n",
    "        temperature=temperature_slider.value,\n",
    "        model=\"palmyra-x-004\",\n",
    "        stream=True\n",
    "    )\n",
    "    output_text = \"\"\n",
    "    for chunk in chat_response:\n",
    "        if chunk.choices[0][\"delta\"][\"content\"]:\n",
    "            output_text += chunk.choices[0][\"delta\"][\"content\"]\n",
    "        else:\n",
    "            continue\n",
    "        completion_text_area.value = output_text\n",
    "\n",
    "    # Reset UI to \"Awaiting user input\" mode\n",
    "    prompt_text_box.disabled = False\n",
    "    temperature_slider.disabled = False\n",
    "    submit_button.disabled = False\n",
    "    submit_button.description = \"Submit\"\n",
    "    submit_button.icon = \"upload\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    prompt_text_box, temperature_slider, submit_button, completion_text_area = display_ui()\n",
    "    submit_button.on_click(lambda button: generate_completion(prompt_text_box, temperature_slider, submit_button, completion_text_area))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971c11c-3852-4611-9947-915aaa30a713",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "#### A different argument for the `chat()` method\n",
    "\n",
    "This version of the app calls the `chat()` method in pretty much the same way with one notable exception: the argument it provides for the `stream` parameter is `True`, which specifies that Palmyra should stream its responses as it generates them rather than waiting until the response has been completely generated before returning it:\n",
    "\n",
    "```\n",
    "chat_response = client.chat.chat(\n",
    "    messages=[initial_system_message, user_message],\n",
    "    model=\"palmyra-x-004\",\n",
    "    stream=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### The response to the `chat()` method\n",
    "\n",
    "When `chat()` is called with its `stream` parameter set to `True`, it returns a stream of `ChatStreamingData` objects with properties that differ slightly from when `stream` parameter set to `False`:\n",
    "\n",
    "<table  width=\"66%\">\n",
    "    <tr>\n",
    "        <th width=\"25%\" style=\"background-color: #5551ff; color: #ffffff;\">Property</th>\n",
    "        <th style=\"background-color: #5551ff; color: #ffffff;\">Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>id</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">A string containing the <code>Chat</code> object’s unique identifier.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>choices</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            A list of dictionaries representing possible portions of the completion (usually containing just one). WIth streaming completions, the key that matters is <code>\"delta\"</code>, whose corresponding value is a small piece of the completion. It’s a dictionary with the keys <code>\"content\"</code> and <code>\"role\"</code>, which serve the same purpose as the <code>\"content\"</code> and <code>\"role\"</code> keys in a message dictionary.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>created</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">An integer representing the time when the response was created as a Unix timestamp. You can use this to compare the timing of the response with timings of other events.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>model</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">A string specifying the model that generated the response.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Why does this version use Jupyter Widgets?\n",
    "\n",
    "It _is_ possible to simply use a `print()` function to display the response stream — it’s as simple as this:\n",
    "\n",
    "```\n",
    "for chunk in chat_response:\n",
    "    print(chunk.choices[0][\"delta\"][\"content\"])\n",
    "```\n",
    "\n",
    "The `for` loop continues as long as the stream hasn’t finished, with each iteration happening when Palmyra generates the next part of its response. The problem is that if you use the `print()` function to display the chunks as they arrive, you get output that looks like this:\n",
    "\n",
    "```\n",
    " The\n",
    " fastest\n",
    " land\n",
    " animal\n",
    " is\n",
    " the\n",
    " che\n",
    "et\n",
    "ah\n",
    ".\n",
    " It\n",
    " can\n",
    " reach\n",
    " speeds\n",
    " up\n",
    " to\n",
    " \n",
    "7\n",
    "0\n",
    "-\n",
    "7\n",
    "5\n",
    " miles\n",
    " per\n",
    " hour\n",
    "...\n",
    "```\n",
    "\n",
    "The solution to this problem is to feed the stream into a UI component whose contents can be updated in real time. Fortunately, there’s Jupyter Widgets, a library that brings UI widgets to Jupyter Notebooks so that they can be as interactive as web and desktop applications.\n",
    "\n",
    "#### Drawing the UI\n",
    "\n",
    "The `display_ui()` function creates four UI objects, or widgets:\n",
    "\n",
    "- `prompt_text_box`: A text box where the user enters their prompt.\n",
    "- `temperature_slider`: A slider that lets the user can set the temperature within a range of 0.0 to 2.0 in increments of 0.1.\n",
    "- `submit_button`: The user clicks this button to submit their prompt.\n",
    "- `completion_text_area`: Palymra’s response appears here.\n",
    "\n",
    "The widgets are laid out inside a `VBox` layout container.\n",
    "\n",
    "`display_ui()` also returns `prompt_text_box`, `temperature_slider`, `submit_button`, and `completion_text_area` widgets so that they can be referenced by other code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca340c91-0f92-4136-93b6-2ce9cb4738d0",
   "metadata": {},
   "source": [
    "<a id=\"multi-turn-chat-completion-streaming-version\"></a>\n",
    "## **Multi-turn chat completion (streaming version)**\n",
    "\n",
    "Here’s the streaming version of the multi-turn chat app. Like the streaming single-turn app, this application uses Jupyter Widgets. Note that this app has two text areas: one for the most recent response from Palmyra, and another one below it that contains the text of the entire conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45d123-ba54-4e13-ae54-f62a9c67a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, ButtonStyle, FloatSlider, HBox, Layout, Text, Textarea, VBox\n",
    "\n",
    "def display_ui():\n",
    "    prompt_text_box = Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt here\",\n",
    "        description=\"Prompt:\",\n",
    "        style={\"description_width\": \"150px\"},\n",
    "        layout=Layout(width=\"500px\"),\n",
    "        continuous_update=False,\n",
    "        disabled=False   \n",
    "    )\n",
    "    temperature_slider = FloatSlider(\n",
    "        min=0.0,\n",
    "        max=2.0,\n",
    "        value=1.0,\n",
    "        step=0.1,\n",
    "        orientation=\"horizontal\",\n",
    "        description=\"Temperature:\",\n",
    "        style={\"description_width\": \"150px\"},\n",
    "        layout=Layout(width=\"500px\"),\n",
    "        continuous_update=False,\n",
    "        readout=True,\n",
    "        readout_format=\".1f\",\n",
    "        disabled=False\n",
    "    )\n",
    "    submit_button = Button(\n",
    "        description=\"Submit\",\n",
    "        tooltip=\"Click me\",\n",
    "        style=ButtonStyle(button_color=\"thistle\", font_weight=\"bold\"),\n",
    "        layout = Layout(margin='0px 0px 0px 160px'),\n",
    "        icon=\"upload\",\n",
    "        disabled=False\n",
    "    )\n",
    "    completion_text_area = Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"\",\n",
    "        description=\"Current\\nresponse:\",\n",
    "        style={\"description_width\": \"150px\"},\n",
    "        layout=Layout(width=\"800px\", height=\"200px\", margin=\"10px 0px 10px 0px\"),\n",
    "        disabled=False\n",
    "    )\n",
    "    conversation_text_area = Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"\",\n",
    "        description=\"Full\\nconversation:\",\n",
    "        style={\"description_width\": \"150px\"},\n",
    "        layout=Layout(width=\"800px\", height=\"200px\", margin=\"10px 0px 10px 0px\"),\n",
    "        disabled=False\n",
    "    )\n",
    "    display(\n",
    "        VBox(\n",
    "            [\n",
    "                prompt_text_box,\n",
    "                temperature_slider,\n",
    "                submit_button,\n",
    "                completion_text_area,\n",
    "                conversation_text_area\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return (prompt_text_box, temperature_slider, submit_button, completion_text_area, conversation_text_area)\n",
    "    \n",
    "def generate_completion(prompt_text_box, temperature_slider, submit_button, completion_text_area, conversation_text_area, messages): \n",
    "    # Add user prompt to messages\n",
    "    user_prompt = prompt_text_box.value.strip()\n",
    "    conversation_text_area.value += f\"{user_prompt}\\n\\n\"\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Put UI in \"generating\" mode\n",
    "    prompt_text_box.disabled = True\n",
    "    temperature_slider.disabled = True\n",
    "    submit_button.disabled = True\n",
    "    submit_button.description = \"Generating...\"\n",
    "    submit_button.icon = \"hourglass\"\n",
    "\n",
    "    # Generate completion and display it\n",
    "    temperature = temperature_slider.value\n",
    "    chat_response = client.chat.chat(\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        model=\"palmyra-x-004\",\n",
    "        stream=True\n",
    "    )\n",
    "    output_text = \"\"\n",
    "    for chunk in chat_response:\n",
    "        print(f\"chunk type: {type(chunk)}\")\n",
    "        if chunk.choices[0][\"delta\"][\"content\"]:\n",
    "            output_text += chunk.choices[0][\"delta\"][\"content\"]\n",
    "            completion_text_area.value = output_text\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Add Palmyra’s response to the `messages` list\n",
    "    model_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": output_text\n",
    "    }\n",
    "    messages.append(model_message)\n",
    "\n",
    "    # Reset UI to \"Awaiting user input\" mode\n",
    "    prompt_text_box.value = \"\"\n",
    "    prompt_text_box.disabled = False\n",
    "    temperature_slider.disabled = False\n",
    "    submit_button.disabled = False\n",
    "    submit_button.description = \"Submit\"\n",
    "    submit_button.icon = \"upload\"\n",
    "\n",
    "    # Update conversation text area\n",
    "    conversation_text_area.value += f\"[Temperature: {temperature:.1f}]\\n{output_text}\\n\\n\"\n",
    "\n",
    "initial_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. Respond concisely and politely to user queries. Use clear, simple language. When asked for technical explanations, provide detailed and accurate information, but avoid jargon. If the user asks for assistance with a task, offer step-by-step guidance.\"\n",
    "}\n",
    "messages = [initial_system_message]\n",
    "\n",
    "prompt_text_box, temperature_slider, submit_button, completion_text_area, conversation_text_area = display_ui()\n",
    "submit_button.on_click(lambda button: generate_completion(prompt_text_box, temperature_slider, submit_button, completion_text_area, conversation_text_area, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0385acd-8022-484f-8e47-325e419853e5",
   "metadata": {},
   "source": [
    "<a id=\"for-more-information\"></a>\n",
    "## **For more information**\n",
    "\n",
    "For more information about chat completions, the `chat` object, and its `chat()` method, see:\n",
    "\n",
    "- [The _Chat completion_ guide](https://dev.writer.com/api-guides/chat-completion)\n",
    "- [The completion API’s _Chat completion page](https://dev.writer.com/api-guides/api-reference/completion-api/chat-completion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
