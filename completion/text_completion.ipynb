{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac40466-9eeb-4804-b9af-f90f9aa45666",
   "metadata": {},
   "source": [
    "# **Text completions with Writer**\n",
    "\n",
    "**_Text completions_** are LLM responses to a user’s request, command or question. They tend to be longer than chat completions, and you can specify a maximum response length. This cookbook shows how to use the `create()` method of Writer’s `completions` object to create apps that provide text completions, with streaming and non-streaming responses.\n",
    "\n",
    "## **Contents**\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Setup](#setup)\n",
    "- [The `completions` object](#the-completions-object)\n",
    "- [Text completion (non-streaming version)](#text-completion-non-streaming-version)\n",
    "- [Text completion (streaming version)](#text-completion-streaming-version)\n",
    "- [For more information](#for-more-information)\n",
    "\n",
    "<a id=\"introduction\"></a>\n",
    "## **Introduction**\n",
    "\n",
    "### What are text completions?\n",
    "\n",
    "**In text completion, the model generates or completes text based on a given prompt.** It’s usually a single-turn process, where the user provides an input and the model generates an output, and the model doesn’t have to maintain context or history over multiple inputs and outputs. It’s “one and done.”\n",
    "\n",
    "Text completion, as its name implies, is often used for completing sentences and paragraphs, but can also be used to generate long-form text content based on a single prompt or some kind of partial input. You may want to think of text completions as longer-form versions of “one-shot” chat completions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3bd93-5046-43be-937e-f2ada6f3e822",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## **Setup**\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "This notebook uses the following packages:\n",
    "\n",
    "* `ipywidgets`: To draw UI widgets for the streaming versions of the apps.\n",
    "* `python-dotenv`: To load environment variables.\n",
    "* `writer-sdk`: To access the Writer API.\n",
    "\n",
    "Run the cell below ensure you have these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f24ef-e845-406a-9f11-a1fb8e189716",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253d278-ed14-4270-b639-d5934f8f0e0e",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "The cell below performs the initialization required for this notebook including the creation of an instance of the `Writer` object to interact with the LLM.\n",
    "\n",
    "To create a Writer client object, you need an API key. [You can sign up for one for free](https://app.writer.com/aistudio/signup). \n",
    "\n",
    "Once you have an API key, we recommend that you store it as an environment variable in a `.env` file like so:\n",
    "\n",
    "```\n",
    "WRITER_API_KEY=\"{Your Writer API key goes here}\"\n",
    "```\n",
    "\n",
    "When you instantiate the client with `client = Writer()`, the newly-created object will automatically look for an environment variable named `WRITER_API_KEY` and will complete the instantiation if an only if `WRITER_API_KEY` has been defined. This notebook uses the [python-dotenv](https://pypi.org/project/python-dotenv/) library to automatically define environment variables based on the contents of an `.env` file in the same directory.\n",
    "\n",
    "The `Writer()` initializer method also has an `api_key` parameter that you can use like this...\n",
    "\n",
    "```\n",
    "client = Writer(api_key=\"{Your Writer API key goes here}\")\n",
    "```\n",
    "\n",
    "...but we strongly encourage you not to leave API keys in your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf3075-f997-4602-97a7-676c08350dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before running any other cells in this cookbook!\n",
    "\n",
    "from writerai import Writer\n",
    "\n",
    "# Load environment variables from .env file\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "client = Writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885cc837-1d49-41f9-915e-65bf31faf9d7",
   "metadata": {},
   "source": [
    "<a id=\"the-completions-object\"></a>\n",
    "## **The `completions` object**\n",
    "\n",
    "Now that you have a Writer client instance, it’s time to start building text completion apps! \n",
    "\n",
    "The `completions` property of a Writer client instance contains methods and properties related to text completion. In all the examples in this cookbook, you’ll build text completion apps by using the `completions` property’s `create()` method, which makes requests for text completions from Palmyra, Writer's custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f673dd-6d7a-4e80-bf86-be3d6ed5ac89",
   "metadata": {},
   "source": [
    "<a id=\"text-completion-non-streaming-version\"></a>\n",
    "## **Text completion (non-streaming version)**\n",
    "\n",
    "The cell below contains a simple one-shot text completion app. When you run it, you will be asked to enter a prompt. After you enter the prompt, you can expect to wait a few moments while Palmyra generates the complete text of its response. Once Palmyra’s done generating, the app will display the response and finish running.\n",
    "\n",
    "Try entering a question or command that can be answered or satisfied with a few short paragraphs (e.g. “Tell me a story”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6612a-ac64-4a42-9746-9249a6fa2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Sample text completion app\n",
    "==========================\n",
    "\"\"\")\n",
    "\n",
    "# Get prompt, temperature, and max tokens from user\n",
    "while True:\n",
    "    prompt = input(\"Enter a prompt: \").strip()\n",
    "    if prompt:\n",
    "        break\n",
    "    print(\"Please provide a prompt to continue.\\n\")\n",
    "while True:\n",
    "    temperature = input(\"Enter the temperature (default: 1.0): \").strip()\n",
    "    if not temperature:\n",
    "        temperature = 1.0\n",
    "        break\n",
    "    try:\n",
    "        temperature = float(temperature)\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Please provide a valid number.\\n\")\n",
    "while True:\n",
    "    max_tokens = input(\"Enter the maximum number of tokens to generate (default: 2048): \").strip()\n",
    "    if not max_tokens:\n",
    "        max_tokens = 2048\n",
    "        break\n",
    "    if max_tokens.isdigit():\n",
    "        max_tokens = int(max_tokens)\n",
    "        break\n",
    "    print(\"Please provide a valid number.\\n\")\n",
    "\n",
    "# Generate and display text completion\n",
    "completion = client.completions.create(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    "    model=\"palmyra-x-004\",\n",
    "    stream=False\n",
    ")\n",
    "print(f\"\\n{completion.choices[0].text}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a741569-0704-435b-a72c-1c968cb97833",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "#### Calling the `create()` method\n",
    "\n",
    "The heart of the one-shot chat completion app is this line:\n",
    "\n",
    "```python\n",
    "completion = client.completions.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0.8,\n",
    "    max_tokens=max_tokens,\n",
    "    model=\"palmyra-x-004\",\n",
    "    stream=False\n",
    ")\n",
    "```\n",
    "\n",
    "It makes a call to the client instance’s `completions` object’s `create()` method, which requests a text completion from Palmyra. In order to get that completion, the call provides arguments for the following parameters:\n",
    "\n",
    "<table width=\"66%\">\n",
    "    <tr>\n",
    "        <th width=\"25%\" style=\"background-color: #5551ff; color: #ffffff;\">Parameter</th>\n",
    "        <th style=\"background-color: #5551ff; color: #ffffff;\">Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>prompt</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            The input that Palmyra will use as a basis for the completion it will return as its response.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>temperature</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            <p>A float that controls the level of randomness in the text that Palmyra generates:</p>\n",
    "            <ul>\n",
    "                <li>The default value is 1.</li>\n",
    "                <li>At temperatures <em>below</em> 1, the responses are more deterministic and predictable, with Palmyra tending to choose the highest probability tokens based on previously-generated ones. The generated output is predictable and repetitive, and produces more \"safe\" or \"obvious\" answers.</li>\n",
    "                <li>At temperatures <em>above</em> 1, the responses are more random and “imaginative,”  with Palmyra giving less probable tokens a better chance of being chosen. The generated output is less predictable, and produces more “creative” answers. The results become increasingly nonsensical at temperatures of about 1.4 and higher, especially for longer completions.</li>\n",
    "            </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>max_tokens</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            An integer specifying the maximum number of tokens that Palmyra can generate in its response. The default value is 2048.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>model</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            A string specifying which model to use. In this case, we’re using the latest model\n",
    "            at the time of writing, <code>palmyra-x-004</code>.\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid #bfcbff;\"><code>stream</code></td>\n",
    "        <td style=\"border: 1px solid #bfcbff;\">\n",
    "            A boolean specifying if the method should stream the completion in chunks\n",
    "            in real time as Palmyra generates it (<code>True</code>) or wait until Palmyra\n",
    "            finishes generating the completion before returning a value (<code>False</code>).\n",
    "            Since we want the completion all at once, we set this parameter to <code>False</code>.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ebde08-bff6-4a07-8213-4821eb0037bf",
   "metadata": {},
   "source": [
    "<a id=\"text-completion-streaming-version\"></a>\n",
    "## **Text completion (streaming version)**\n",
    "\n",
    "The cell below is a _streaming_ version of the text completion app. With this app, after you enter the prompt, you will immediately see the completion as Palmyra generates it as a stream of text in a manner similar to a lot of AI chat apps.\n",
    "\n",
    "Note that this version of the app uses the [Jupyter Widgets](https://ipywidgets.readthedocs.io/en/stable/) library to provide a graphical user interface for the app. There’s a reason, which will be explained in the notes in the cell after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba4151-f672-4201-8d05-947821eb7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, ButtonStyle, FloatSlider, HBox, IntSlider, Layout, Text, Textarea, VBox\n",
    "\n",
    "def display_ui():\n",
    "    prompt_text_box = Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt here\",\n",
    "        description=\"Prompt:\",\n",
    "        layout=Layout(width=\"500px\", margin=\"0px 0px 0px 25px\"),\n",
    "        continuous_update=False,\n",
    "        disabled=False   \n",
    "    )\n",
    "    temperature_slider = FloatSlider(\n",
    "        min=0.0,\n",
    "        max=2.0,\n",
    "        value=1.0,\n",
    "        step=0.1,\n",
    "        orientation=\"horizontal\",\n",
    "        description=\"Temperature:\",\n",
    "        style={\"description_width\": \"100px\"},\n",
    "        layout=Layout(width=\"500px\"),\n",
    "        continuous_update=False,\n",
    "        readout=True,\n",
    "        readout_format=\".1f\",\n",
    "        disabled=False\n",
    "    )\n",
    "    max_tokens_slider = IntSlider(\n",
    "        min=0,\n",
    "        max=16336,\n",
    "        value=2048,\n",
    "        step=256,\n",
    "        orientation=\"horizontal\",\n",
    "        description=\"Max tokens:\",\n",
    "        style={\"description_width\": \"100px\"},\n",
    "        layout=Layout(width=\"500px\"),\n",
    "        continuous_update=False,\n",
    "        readout=True,\n",
    "        disabled=False\n",
    "    )\n",
    "    submit_button = Button(\n",
    "        description=\"Submit\",\n",
    "        tooltip=\"Click me\",\n",
    "        style=ButtonStyle(button_color=\"thistle\", font_weight=\"bold\"),\n",
    "        layout = Layout(margin=\"0px 0px 0px 115px\"),\n",
    "        icon=\"upload\",\n",
    "        disabled=False\n",
    "    )\n",
    "    completion_text_area = Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"\",\n",
    "        description=\"Response:\",\n",
    "        layout=Layout(width=\"800px\", height=\"200px\", margin=\"10px 0px 0px 25px\"),\n",
    "        disabled=False\n",
    "    )\n",
    "    display(\n",
    "        VBox(\n",
    "            [\n",
    "                prompt_text_box,\n",
    "                temperature_slider,\n",
    "                max_tokens_slider,\n",
    "                submit_button,\n",
    "                completion_text_area,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return (prompt_text_box, temperature_slider, max_tokens_slider, submit_button, completion_text_area)\n",
    "\n",
    "def generate_completion(prompt_text_box, temperature_slider, max_tokens_slider, submit_button, completion_text_area):\n",
    "    # Put UI in \"generating\" mode\n",
    "    prompt_text_box.disabled = True\n",
    "    temperature_slider.disabled = True\n",
    "    max_tokens_slider.disabled = True\n",
    "    submit_button.disabled = True\n",
    "    submit_button.description = \"Generating...\"\n",
    "    submit_button.icon = \"hourglass\"\n",
    "    completion_text_area.value = \"\"\n",
    "\n",
    "    # Generate completion and display it\n",
    "    completion = client.completions.create(\n",
    "      prompt=prompt_text_box.value.strip(),\n",
    "      temperature=temperature_slider.value,\n",
    "      max_tokens=max_tokens_slider.value,\n",
    "      model=\"palmyra-x-004\",\n",
    "      stream=True\n",
    "    )\n",
    "    output_text = \"\"\n",
    "    for chunk in completion:\n",
    "        if chunk.value is None:\n",
    "            continue\n",
    "        else:\n",
    "            completion_text_area.value += chunk.value\n",
    "\n",
    "    # Reset UI to \"Awaiting user input\" mode\n",
    "    prompt_text_box.disabled = False\n",
    "    temperature_slider.disabled = False\n",
    "    max_tokens_slider.disabled = False\n",
    "    submit_button.disabled = False\n",
    "    submit_button.description = \"Submit\"\n",
    "    submit_button.icon = \"upload\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    prompt_text_box, temperature_slider, max_tokens_slider, submit_button, completion_text_area = display_ui()\n",
    "    submit_button.on_click(lambda button: generate_completion(prompt_text_box, temperature_slider, max_tokens_slider, submit_button, completion_text_area))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4d37f-94ab-4ab7-bd1f-a4e3d03017c5",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "#### A different argument for the `create()` method\n",
    "\n",
    "This version of the app calls the `create()` method in pretty much the same way with one notable exception: the argument it provides for the `stream` parameter is `True`, which specifies that Palmyra should stream its responses as it generates them rather than waiting until the response has been completely generated before returning it:\n",
    "\n",
    "```python\n",
    "completion = client.completions.create(\n",
    "    prompt=prompt_text_box.value.strip(),\n",
    "    temperature=temperature_slider.value,\n",
    "    max_tokens=max_tokens_slider.value,\n",
    "    model=\"palmyra-x-004\",\n",
    "    stream=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### Why does this version use Jupyter Widgets?\n",
    "\n",
    "It _is_ possible to simply use a `print()` function to display the response stream — it’s as simple as this:\n",
    "\n",
    "```\n",
    "for chunk in completion:\n",
    "    print(chunk.value)\n",
    "```\n",
    "\n",
    "The `for` loop continues as long as the stream hasn’t finished, with each iteration happening when Palmyra generates the next part of its response. The problem is that if you use the `print()` function to display the chunks as they arrive, you get output that looks like this:\n",
    "\n",
    "```\n",
    " Sure\n",
    ",\n",
    " I\n",
    "'\n",
    "d\n",
    " be\n",
    " happy\n",
    " to\n",
    " share\n",
    " a\n",
    " story\n",
    "!\n",
    " Once\n",
    " upon\n",
    " a\n",
    " time\n",
    " in\n",
    " a\n",
    " small\n",
    " town\n",
    " nest\n",
    "led\n",
    " between\n",
    " rolling\n",
    " hills\n",
    " and\n",
    " a\n",
    " spark\n",
    "ling\n",
    " river\n",
    ",\n",
    "```\n",
    "\n",
    "The solution to this problem is to feed the stream into a UI component whose contents can be updated in real time. Fortunately, there’s Jupyter Widgets, a library that brings UI widgets to Jupyter Notebooks so that they can be as interactive as web and desktop applications.\n",
    "\n",
    "#### Drawing the UI\n",
    "\n",
    "The `display_ui()` function creates three UI objects:\n",
    "\n",
    "- `prompt_text_box`: A text box where the user enters their prompt.\n",
    "- `submit_button`: The user clicks this button to submit their prompt.\n",
    "- `completion_text_area`: Palymra’s response appears here.\n",
    "\n",
    "The `prompt_text_box` and `submit_button` are laid out inside an `HBox` layout container, which in turn is laid out with `completion_text_area` inside a `VBox` layout container.\n",
    "\n",
    "`display_ui()` also returns `prompt_text_box`, `submit_button`, `completion_text_area` widgets so that they can be referenced by other code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca0fee",
   "metadata": {},
   "source": [
    "<a id=\"for-more-information\"></a>\n",
    "## **For more information**\n",
    "\n",
    "For more information about chat completions, the `chat` object, and its `chat()` method, see:\n",
    "\n",
    "- [The _Text generation_ guide](https://dev.writer.com/api-guides/text-generation)\n",
    "- [The completion API’s _Text generation_ page](https://dev.writer.com/api-guides/api-reference/completion-api/text-generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
